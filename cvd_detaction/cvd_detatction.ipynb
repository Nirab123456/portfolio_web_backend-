{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>bp_category</th>\n",
       "      <th>bp_category_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68200</th>\n",
       "      <td>19240</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68201</th>\n",
       "      <td>22601</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68202</th>\n",
       "      <td>19066</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68203</th>\n",
       "      <td>22431</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68204</th>\n",
       "      <td>20540</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68205 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0      18393       2     168    62.0    110     80            1     1      0   \n",
       "1      20228       1     156    85.0    140     90            3     1      0   \n",
       "2      18857       1     165    64.0    130     70            3     1      0   \n",
       "3      17623       2     169    82.0    150    100            1     1      0   \n",
       "4      17474       1     156    56.0    100     60            1     1      0   \n",
       "...      ...     ...     ...     ...    ...    ...          ...   ...    ...   \n",
       "68200  19240       2     168    76.0    120     80            1     1      1   \n",
       "68201  22601       1     158   126.0    140     90            2     2      0   \n",
       "68202  19066       2     183   105.0    180     90            3     1      0   \n",
       "68203  22431       1     163    72.0    135     80            1     2      0   \n",
       "68204  20540       1     170    72.0    120     80            2     1      0   \n",
       "\n",
       "       alco  active           bp_category   bp_category_encoded  \n",
       "0         0       1  Hypertension Stage 1  Hypertension Stage 1  \n",
       "1         0       1  Hypertension Stage 2  Hypertension Stage 2  \n",
       "2         0       0  Hypertension Stage 1  Hypertension Stage 1  \n",
       "3         0       1  Hypertension Stage 2  Hypertension Stage 2  \n",
       "4         0       0                Normal                Normal  \n",
       "...     ...     ...                   ...                   ...  \n",
       "68200     0       1  Hypertension Stage 1  Hypertension Stage 1  \n",
       "68201     0       1  Hypertension Stage 2  Hypertension Stage 2  \n",
       "68202     1       0  Hypertension Stage 2  Hypertension Stage 2  \n",
       "68203     0       0  Hypertension Stage 1  Hypertension Stage 1  \n",
       "68204     0       1  Hypertension Stage 1  Hypertension Stage 1  \n",
       "\n",
       "[68205 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('cardio_data_processed.csv')\n",
    "df.drop(columns='age_years', inplace=True)\n",
    "df.drop(columns='bmi', inplace=True)\n",
    "df.drop(columns='cardio', inplace=True)\n",
    "df.drop(columns='id', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to normalize\n",
    "features_to_normalize = ['age','gender', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "# Extract the feature columns from the dataframe\n",
    "X = df[features_to_normalize]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            age  gender    height    weight     ap_hi     ap_lo  cholesterol  \\\n",
      "0      0.588076     1.0  0.579487  0.269841  0.222222  0.333333          0.0   \n",
      "1      0.730159     0.0  0.517949  0.391534  0.555556  0.500000          1.0   \n",
      "2      0.624003     0.0  0.564103  0.280423  0.444444  0.166667          1.0   \n",
      "3      0.528455     1.0  0.584615  0.375661  0.666667  0.666667          0.0   \n",
      "4      0.516918     0.0  0.517949  0.238095  0.111111  0.000000          0.0   \n",
      "...         ...     ...       ...       ...       ...       ...          ...   \n",
      "68200  0.653659     1.0  0.579487  0.343915  0.333333  0.333333          0.0   \n",
      "68201  0.913899     0.0  0.528205  0.608466  0.555556  0.500000          0.5   \n",
      "68202  0.640186     1.0  0.656410  0.497354  1.000000  0.500000          1.0   \n",
      "68203  0.900736     0.0  0.553846  0.322751  0.500000  0.333333          0.0   \n",
      "68204  0.754317     0.0  0.589744  0.322751  0.333333  0.333333          0.5   \n",
      "\n",
      "       gluc  smoke  alco  active  \n",
      "0       0.0    0.0   0.0     1.0  \n",
      "1       0.0    0.0   0.0     1.0  \n",
      "2       0.0    0.0   0.0     0.0  \n",
      "3       0.0    0.0   0.0     1.0  \n",
      "4       0.0    0.0   0.0     0.0  \n",
      "...     ...    ...   ...     ...  \n",
      "68200   0.0    1.0   0.0     1.0  \n",
      "68201   0.5    0.0   0.0     1.0  \n",
      "68202   0.0    0.0   1.0     0.0  \n",
      "68203   0.5    0.0   0.0     0.0  \n",
      "68204   0.0    0.0   0.0     1.0  \n",
      "\n",
      "[68205 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame for better readability\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=features_to_normalize)\n",
    "\n",
    "# Show the normalized data\n",
    "print(X_normalized_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hypertension Stage 1', 'Hypertension Stage 2', 'Normal',\n",
       "       'Elevated'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bp_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min': [10798.0, 1.0, 55.0, 11.0, 90.0, 60.0, 1.0, 1.0, 0.0, 0.0, 0.0], 'scale': [12915.0, 1.0, 195.0, 189.0, 90.0, 60.0, 2.0, 2.0, 1.0, 1.0, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "# Extract the min and scale values from the scaler\n",
    "scaler_params = {\n",
    "    'min': scaler.data_min_.tolist(),   # The minimum values for each feature\n",
    "    'scale': scaler.data_range_.tolist() # The scaling factors for each feature\n",
    "}\n",
    "\n",
    "# Save the parameters as a JSON file\n",
    "import json\n",
    "\n",
    "with open('minmax_scaler_params.json', 'w') as json_file:\n",
    "    json.dump(scaler_params, json_file)\n",
    "\n",
    "print(scaler_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
      "0      18393       2     168    62.0    110     80            1     1      0   \n",
      "1      20228       1     156    85.0    140     90            3     1      0   \n",
      "2      18857       1     165    64.0    130     70            3     1      0   \n",
      "3      17623       2     169    82.0    150    100            1     1      0   \n",
      "4      17474       1     156    56.0    100     60            1     1      0   \n",
      "...      ...     ...     ...     ...    ...    ...          ...   ...    ...   \n",
      "68200  19240       2     168    76.0    120     80            1     1      1   \n",
      "68201  22601       1     158   126.0    140     90            2     2      0   \n",
      "68202  19066       2     183   105.0    180     90            3     1      0   \n",
      "68203  22431       1     163    72.0    135     80            1     2      0   \n",
      "68204  20540       1     170    72.0    120     80            2     1      0   \n",
      "\n",
      "       alco  active           bp_category  bp_category_encoded  \n",
      "0         0       1  Hypertension Stage 1                    0  \n",
      "1         0       1  Hypertension Stage 2                    1  \n",
      "2         0       0  Hypertension Stage 1                    0  \n",
      "3         0       1  Hypertension Stage 2                    1  \n",
      "4         0       0                Normal                    2  \n",
      "...     ...     ...                   ...                  ...  \n",
      "68200     0       1  Hypertension Stage 1                    0  \n",
      "68201     0       1  Hypertension Stage 2                    1  \n",
      "68202     1       0  Hypertension Stage 2                    1  \n",
      "68203     0       0  Hypertension Stage 1                    0  \n",
      "68204     0       1  Hypertension Stage 1                    0  \n",
      "\n",
      "[68205 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a mapping dictionary for bp_category\n",
    "bp_category_mapping = {label: idx for idx, label in enumerate(df['bp_category'].unique())}\n",
    "\n",
    "# Step 2: Encode the 'bp_category' column using the mapping\n",
    "df['bp_category_encoded'] = df['bp_category'].map(bp_category_mapping)\n",
    "\n",
    "# Show the encoded dataframe\n",
    "print(df)\n",
    "\n",
    "# Step 3: Save the mapping as a JSON file\n",
    "with open('bp_category_mapping.json', 'w') as json_file:\n",
    "    json.dump(bp_category_mapping, json_file)\n",
    "\n",
    "# Optionally, save the dataframe with the encoded column if needed\n",
    "df.to_csv('encoded_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.588076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.391534</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.624003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.280423</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.375661</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.516918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68200</th>\n",
       "      <td>0.653659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.343915</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68201</th>\n",
       "      <td>0.913899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528205</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68202</th>\n",
       "      <td>0.640186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656410</td>\n",
       "      <td>0.497354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68203</th>\n",
       "      <td>0.900736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68204</th>\n",
       "      <td>0.754317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68205 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  gender    height    weight     ap_hi     ap_lo  cholesterol  \\\n",
       "0      0.588076     1.0  0.579487  0.269841  0.222222  0.333333          0.0   \n",
       "1      0.730159     0.0  0.517949  0.391534  0.555556  0.500000          1.0   \n",
       "2      0.624003     0.0  0.564103  0.280423  0.444444  0.166667          1.0   \n",
       "3      0.528455     1.0  0.584615  0.375661  0.666667  0.666667          0.0   \n",
       "4      0.516918     0.0  0.517949  0.238095  0.111111  0.000000          0.0   \n",
       "...         ...     ...       ...       ...       ...       ...          ...   \n",
       "68200  0.653659     1.0  0.579487  0.343915  0.333333  0.333333          0.0   \n",
       "68201  0.913899     0.0  0.528205  0.608466  0.555556  0.500000          0.5   \n",
       "68202  0.640186     1.0  0.656410  0.497354  1.000000  0.500000          1.0   \n",
       "68203  0.900736     0.0  0.553846  0.322751  0.500000  0.333333          0.0   \n",
       "68204  0.754317     0.0  0.589744  0.322751  0.333333  0.333333          0.5   \n",
       "\n",
       "       gluc  smoke  alco  active  \n",
       "0       0.0    0.0   0.0     1.0  \n",
       "1       0.0    0.0   0.0     1.0  \n",
       "2       0.0    0.0   0.0     0.0  \n",
       "3       0.0    0.0   0.0     1.0  \n",
       "4       0.0    0.0   0.0     0.0  \n",
       "...     ...    ...   ...     ...  \n",
       "68200   0.0    1.0   0.0     1.0  \n",
       "68201   0.5    0.0   0.0     1.0  \n",
       "68202   0.0    0.0   1.0     0.0  \n",
       "68203   0.5    0.0   0.0     0.0  \n",
       "68204   0.0    0.0   0.0     1.0  \n",
       "\n",
       "[68205 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bp_category_encoded</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.588076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.391534</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.624003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.280423</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.375661</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.516918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68200</th>\n",
       "      <td>0</td>\n",
       "      <td>0.653659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.343915</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68201</th>\n",
       "      <td>1</td>\n",
       "      <td>0.913899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528205</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68202</th>\n",
       "      <td>1</td>\n",
       "      <td>0.640186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656410</td>\n",
       "      <td>0.497354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68203</th>\n",
       "      <td>0</td>\n",
       "      <td>0.900736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68204</th>\n",
       "      <td>0</td>\n",
       "      <td>0.754317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68205 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bp_category_encoded       age  gender    height    weight     ap_hi  \\\n",
       "0                        0  0.588076     1.0  0.579487  0.269841  0.222222   \n",
       "1                        1  0.730159     0.0  0.517949  0.391534  0.555556   \n",
       "2                        0  0.624003     0.0  0.564103  0.280423  0.444444   \n",
       "3                        1  0.528455     1.0  0.584615  0.375661  0.666667   \n",
       "4                        2  0.516918     0.0  0.517949  0.238095  0.111111   \n",
       "...                    ...       ...     ...       ...       ...       ...   \n",
       "68200                    0  0.653659     1.0  0.579487  0.343915  0.333333   \n",
       "68201                    1  0.913899     0.0  0.528205  0.608466  0.555556   \n",
       "68202                    1  0.640186     1.0  0.656410  0.497354  1.000000   \n",
       "68203                    0  0.900736     0.0  0.553846  0.322751  0.500000   \n",
       "68204                    0  0.754317     0.0  0.589744  0.322751  0.333333   \n",
       "\n",
       "          ap_lo  cholesterol  gluc  smoke  alco  active  \n",
       "0      0.333333          0.0   0.0    0.0   0.0     1.0  \n",
       "1      0.500000          1.0   0.0    0.0   0.0     1.0  \n",
       "2      0.166667          1.0   0.0    0.0   0.0     0.0  \n",
       "3      0.666667          0.0   0.0    0.0   0.0     1.0  \n",
       "4      0.000000          0.0   0.0    0.0   0.0     0.0  \n",
       "...         ...          ...   ...    ...   ...     ...  \n",
       "68200  0.333333          0.0   0.0    1.0   0.0     1.0  \n",
       "68201  0.500000          0.5   0.5    0.0   0.0     1.0  \n",
       "68202  0.500000          1.0   0.0    0.0   1.0     0.0  \n",
       "68203  0.333333          0.0   0.5    0.0   0.0     0.0  \n",
       "68204  0.333333          0.5   0.0    0.0   0.0     1.0  \n",
       "\n",
       "[68205 rows x 12 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Now, combine the normalized data with the original dataframe\n",
    "# Drop the original columns that are being normalized\n",
    "df_combined = df.drop(columns=features_to_normalize)\n",
    "\n",
    "# Concatenate the normalized dataframe with the original dataframe (excluding normalized columns)\n",
    "df_combined = pd.concat([df_combined, X_normalized_df], axis=1)\n",
    "df_combined.drop(columns='bp_category', inplace=True)\n",
    "\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 54564\n",
      "Validation set size: 6820\n",
      "Test set size: 6821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your combined dataframe is called df_combined\n",
    "# Let's define the features and the target\n",
    "X = df_combined.drop(columns=['bp_category_encoded'])  # Drop the target columns\n",
    "y = df_combined['bp_category_encoded']  # Assuming this is your target column\n",
    "\n",
    "# First split: 80% training, 20% temporary (for validation and testing)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Second split: 50% validation, 50% testing (from the temporary set)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Now you have:\n",
    "# - X_train, y_train: Training set (80% of the original data)\n",
    "# - X_val, y_val: Validation set (10% of the original data)\n",
    "# - X_test, y_test: Test set (10% of the original data)\n",
    "\n",
    "# You can verify the sizes of each split:\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30732</th>\n",
       "      <td>0.876345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574359</td>\n",
       "      <td>0.306878</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11861</th>\n",
       "      <td>0.290205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.456410</td>\n",
       "      <td>0.338624</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58611</th>\n",
       "      <td>0.692218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.502564</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9441</th>\n",
       "      <td>0.397755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.312169</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65348</th>\n",
       "      <td>0.803871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.195767</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37194</th>\n",
       "      <td>0.673171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.291005</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>0.335579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54886</th>\n",
       "      <td>0.914673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.338624</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0.959504</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>0.296632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.543590</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54564 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  gender    height    weight     ap_hi     ap_lo  cholesterol  \\\n",
       "30732  0.876345     0.0  0.574359  0.306878  0.222222  0.166667          0.0   \n",
       "11861  0.290205     0.0  0.456410  0.338624  0.333333  0.333333          0.0   \n",
       "58611  0.692218     0.0  0.502564  0.296296  0.444444  0.333333          0.0   \n",
       "9441   0.397755     0.0  0.605128  0.312169  0.333333  0.166667          0.0   \n",
       "65348  0.803871     0.0  0.441026  0.195767  0.333333  0.333333          0.0   \n",
       "...         ...     ...       ...       ...       ...       ...          ...   \n",
       "37194  0.673171     0.0  0.512821  0.291005  0.555556  0.333333          0.0   \n",
       "6265   0.335579     0.0  0.579487  0.444444  0.444444  0.333333          0.0   \n",
       "54886  0.914673     0.0  0.523077  0.338624  0.444444  0.333333          0.0   \n",
       "860    0.959504     1.0  0.579487  0.365079  0.777778  0.500000          0.0   \n",
       "15795  0.296632     0.0  0.543590  0.428571  0.111111  0.166667          0.0   \n",
       "\n",
       "       gluc  smoke  alco  active  \n",
       "30732   0.0    0.0   0.0     1.0  \n",
       "11861   0.0    0.0   0.0     1.0  \n",
       "58611   0.0    0.0   0.0     1.0  \n",
       "9441    0.0    0.0   0.0     1.0  \n",
       "65348   0.0    0.0   0.0     0.0  \n",
       "...     ...    ...   ...     ...  \n",
       "37194   0.0    0.0   0.0     1.0  \n",
       "6265    0.5    0.0   0.0     1.0  \n",
       "54886   0.0    0.0   0.0     1.0  \n",
       "860     0.0    0.0   0.0     1.0  \n",
       "15795   0.0    0.0   0.0     1.0  \n",
       "\n",
       "[54564 rows x 11 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "\n",
    "X_val.to_csv('X_val.csv', index=False)\n",
    "y_val.to_csv('y_val.csv', index=False)\n",
    "\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert DataFrames to NumPy arrays and then to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # For classification, target must be long (int)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset which pairs the inputs and outputs (features and labels)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create DataLoader for batching and shuffling the training set\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create DataLoader for the validation set (no need for shuffling during evaluation)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "output_size = len(y.unique())  # Number of unique classes\n",
    "print(output_size)\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_save_dir = 'model'\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.4156\n",
      "Validation Loss: 0.2566, Validation Accuracy: 91.39%\n",
      "Epoch [2/200], Loss: 0.2131\n",
      "Validation Loss: 0.0984, Validation Accuracy: 97.55%\n",
      "Epoch [3/200], Loss: 0.0934\n",
      "Validation Loss: 0.0564, Validation Accuracy: 98.78%\n",
      "Epoch [4/200], Loss: 0.0653\n",
      "Validation Loss: 0.0375, Validation Accuracy: 99.08%\n",
      "Epoch [5/200], Loss: 0.0564\n",
      "Validation Loss: 0.0296, Validation Accuracy: 99.33%\n",
      "Epoch [6/200], Loss: 0.0503\n",
      "Validation Loss: 0.0403, Validation Accuracy: 99.15%\n",
      "Epoch [7/200], Loss: 0.0471\n",
      "Validation Loss: 0.0269, Validation Accuracy: 99.40%\n",
      "Epoch [8/200], Loss: 0.0436\n",
      "Validation Loss: 0.0278, Validation Accuracy: 99.37%\n",
      "Epoch [9/200], Loss: 0.0432\n",
      "Validation Loss: 0.0370, Validation Accuracy: 99.03%\n",
      "Epoch [10/200], Loss: 0.0397\n",
      "Validation Loss: 0.0236, Validation Accuracy: 99.44%\n",
      "Epoch [11/200], Loss: 0.0374\n",
      "Validation Loss: 0.0442, Validation Accuracy: 99.25%\n",
      "Epoch [12/200], Loss: 0.0359\n",
      "Validation Loss: 0.0283, Validation Accuracy: 99.18%\n",
      "Epoch [13/200], Loss: 0.0350\n",
      "Validation Loss: 0.0304, Validation Accuracy: 99.50%\n",
      "Epoch [14/200], Loss: 0.0319\n",
      "Validation Loss: 0.0366, Validation Accuracy: 99.27%\n",
      "Epoch [15/200], Loss: 0.0308\n",
      "Validation Loss: 0.0456, Validation Accuracy: 98.89%\n",
      "Epoch [16/200], Loss: 0.0297\n",
      "Validation Loss: 0.0153, Validation Accuracy: 99.55%\n",
      "Epoch [17/200], Loss: 0.0290\n",
      "Validation Loss: 0.0156, Validation Accuracy: 99.65%\n",
      "Epoch [18/200], Loss: 0.0267\n",
      "Validation Loss: 0.0123, Validation Accuracy: 99.75%\n",
      "Epoch [19/200], Loss: 0.0238\n",
      "Validation Loss: 0.0160, Validation Accuracy: 99.63%\n",
      "Epoch [20/200], Loss: 0.0266\n",
      "Validation Loss: 0.0251, Validation Accuracy: 99.43%\n",
      "Epoch [21/200], Loss: 0.0245\n",
      "Validation Loss: 0.0151, Validation Accuracy: 99.66%\n",
      "Epoch [22/200], Loss: 0.0228\n",
      "Validation Loss: 0.0196, Validation Accuracy: 99.44%\n",
      "Epoch [23/200], Loss: 0.0222\n",
      "Validation Loss: 0.0106, Validation Accuracy: 99.74%\n",
      "Epoch [24/200], Loss: 0.0216\n",
      "Validation Loss: 0.0137, Validation Accuracy: 99.66%\n",
      "Epoch [25/200], Loss: 0.0233\n",
      "Validation Loss: 0.0155, Validation Accuracy: 99.66%\n",
      "Epoch [26/200], Loss: 0.0217\n",
      "Validation Loss: 0.0194, Validation Accuracy: 99.40%\n",
      "Epoch [27/200], Loss: 0.0204\n",
      "Validation Loss: 0.0146, Validation Accuracy: 99.65%\n",
      "Epoch [28/200], Loss: 0.0228\n",
      "Validation Loss: 0.0126, Validation Accuracy: 99.75%\n",
      "Epoch [29/200], Loss: 0.0189\n",
      "Validation Loss: 0.0260, Validation Accuracy: 99.55%\n",
      "Epoch [30/200], Loss: 0.0143\n",
      "Validation Loss: 0.0094, Validation Accuracy: 99.71%\n",
      "Epoch [31/200], Loss: 0.0120\n",
      "Validation Loss: 0.0080, Validation Accuracy: 99.75%\n",
      "Epoch [32/200], Loss: 0.0118\n",
      "Validation Loss: 0.0090, Validation Accuracy: 99.74%\n",
      "Epoch [33/200], Loss: 0.0118\n",
      "Validation Loss: 0.0078, Validation Accuracy: 99.79%\n",
      "Epoch [34/200], Loss: 0.0125\n",
      "Validation Loss: 0.0090, Validation Accuracy: 99.82%\n",
      "Epoch [35/200], Loss: 0.0124\n",
      "Validation Loss: 0.0097, Validation Accuracy: 99.78%\n",
      "Epoch [36/200], Loss: 0.0113\n",
      "Validation Loss: 0.0091, Validation Accuracy: 99.77%\n",
      "Epoch [37/200], Loss: 0.0119\n",
      "Validation Loss: 0.0153, Validation Accuracy: 99.65%\n",
      "Epoch [38/200], Loss: 0.0109\n",
      "Validation Loss: 0.0086, Validation Accuracy: 99.75%\n",
      "Epoch [39/200], Loss: 0.0117\n",
      "Validation Loss: 0.0135, Validation Accuracy: 99.63%\n",
      "Epoch [40/200], Loss: 0.0095\n",
      "Validation Loss: 0.0076, Validation Accuracy: 99.82%\n",
      "Epoch [41/200], Loss: 0.0082\n",
      "Validation Loss: 0.0058, Validation Accuracy: 99.87%\n",
      "Epoch [42/200], Loss: 0.0091\n",
      "Validation Loss: 0.0066, Validation Accuracy: 99.85%\n",
      "Epoch [43/200], Loss: 0.0086\n",
      "Validation Loss: 0.0095, Validation Accuracy: 99.74%\n",
      "Epoch [44/200], Loss: 0.0086\n",
      "Validation Loss: 0.0086, Validation Accuracy: 99.79%\n",
      "Epoch [45/200], Loss: 0.0091\n",
      "Validation Loss: 0.0105, Validation Accuracy: 99.72%\n",
      "Epoch [46/200], Loss: 0.0080\n",
      "Validation Loss: 0.0058, Validation Accuracy: 99.90%\n",
      "Epoch [47/200], Loss: 0.0078\n",
      "Validation Loss: 0.0063, Validation Accuracy: 99.87%\n",
      "Epoch [48/200], Loss: 0.0073\n",
      "Validation Loss: 0.0058, Validation Accuracy: 99.85%\n",
      "Epoch [49/200], Loss: 0.0074\n",
      "Validation Loss: 0.0064, Validation Accuracy: 99.85%\n",
      "Epoch [50/200], Loss: 0.0065\n",
      "Validation Loss: 0.0049, Validation Accuracy: 99.87%\n",
      "Epoch [51/200], Loss: 0.0072\n",
      "Validation Loss: 0.0059, Validation Accuracy: 99.88%\n",
      "Epoch [52/200], Loss: 0.0062\n",
      "Validation Loss: 0.0066, Validation Accuracy: 99.85%\n",
      "Epoch [53/200], Loss: 0.0065\n",
      "Validation Loss: 0.0053, Validation Accuracy: 99.87%\n",
      "Epoch [54/200], Loss: 0.0061\n",
      "Validation Loss: 0.0059, Validation Accuracy: 99.85%\n",
      "Epoch [55/200], Loss: 0.0062\n",
      "Validation Loss: 0.0067, Validation Accuracy: 99.85%\n",
      "Epoch [56/200], Loss: 0.0069\n",
      "Validation Loss: 0.0047, Validation Accuracy: 99.85%\n",
      "Epoch [57/200], Loss: 0.0067\n",
      "Validation Loss: 0.0045, Validation Accuracy: 99.88%\n",
      "Epoch [58/200], Loss: 0.0070\n",
      "Validation Loss: 0.0049, Validation Accuracy: 99.91%\n",
      "Epoch [59/200], Loss: 0.0063\n",
      "Validation Loss: 0.0052, Validation Accuracy: 99.87%\n",
      "Epoch [60/200], Loss: 0.0055\n",
      "Validation Loss: 0.0056, Validation Accuracy: 99.88%\n",
      "Epoch [61/200], Loss: 0.0064\n",
      "Validation Loss: 0.0066, Validation Accuracy: 99.84%\n",
      "Epoch [62/200], Loss: 0.0059\n",
      "Validation Loss: 0.0044, Validation Accuracy: 99.88%\n",
      "Epoch [63/200], Loss: 0.0066\n",
      "Validation Loss: 0.0063, Validation Accuracy: 99.85%\n",
      "Epoch [64/200], Loss: 0.0061\n",
      "Validation Loss: 0.0047, Validation Accuracy: 99.90%\n",
      "Epoch [65/200], Loss: 0.0060\n",
      "Validation Loss: 0.0061, Validation Accuracy: 99.87%\n",
      "Epoch [66/200], Loss: 0.0060\n",
      "Validation Loss: 0.0062, Validation Accuracy: 99.85%\n",
      "Epoch [67/200], Loss: 0.0064\n",
      "Validation Loss: 0.0059, Validation Accuracy: 99.84%\n",
      "Epoch [68/200], Loss: 0.0068\n",
      "Validation Loss: 0.0049, Validation Accuracy: 99.87%\n",
      "Epoch [69/200], Loss: 0.0053\n",
      "Validation Loss: 0.0063, Validation Accuracy: 99.85%\n",
      "Epoch [70/200], Loss: 0.0059\n",
      "Validation Loss: 0.0056, Validation Accuracy: 99.88%\n",
      "Epoch [71/200], Loss: 0.0052\n",
      "Validation Loss: 0.0060, Validation Accuracy: 99.85%\n",
      "Epoch [72/200], Loss: 0.0053\n",
      "Validation Loss: 0.0052, Validation Accuracy: 99.87%\n",
      "Epoch [73/200], Loss: 0.0057\n",
      "Validation Loss: 0.0054, Validation Accuracy: 99.88%\n",
      "Epoch [74/200], Loss: 0.0053\n",
      "Validation Loss: 0.0069, Validation Accuracy: 99.84%\n",
      "Epoch [75/200], Loss: 0.0054\n",
      "Validation Loss: 0.0054, Validation Accuracy: 99.88%\n",
      "Epoch [76/200], Loss: 0.0047\n",
      "Validation Loss: 0.0052, Validation Accuracy: 99.87%\n",
      "Epoch [77/200], Loss: 0.0050\n",
      "Validation Loss: 0.0046, Validation Accuracy: 99.88%\n",
      "Epoch [78/200], Loss: 0.0049\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [79/200], Loss: 0.0052\n",
      "Validation Loss: 0.0056, Validation Accuracy: 99.88%\n",
      "Epoch [80/200], Loss: 0.0054\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [81/200], Loss: 0.0055\n",
      "Validation Loss: 0.0052, Validation Accuracy: 99.88%\n",
      "Epoch [82/200], Loss: 0.0054\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [83/200], Loss: 0.0048\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [84/200], Loss: 0.0049\n",
      "Validation Loss: 0.0057, Validation Accuracy: 99.88%\n",
      "Epoch [85/200], Loss: 0.0047\n",
      "Validation Loss: 0.0049, Validation Accuracy: 99.88%\n",
      "Epoch [86/200], Loss: 0.0050\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [87/200], Loss: 0.0049\n",
      "Validation Loss: 0.0053, Validation Accuracy: 99.88%\n",
      "Epoch [88/200], Loss: 0.0050\n",
      "Validation Loss: 0.0055, Validation Accuracy: 99.88%\n",
      "Epoch [89/200], Loss: 0.0045\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [90/200], Loss: 0.0044\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [91/200], Loss: 0.0043\n",
      "Validation Loss: 0.0054, Validation Accuracy: 99.88%\n",
      "Epoch [92/200], Loss: 0.0048\n",
      "Validation Loss: 0.0052, Validation Accuracy: 99.88%\n",
      "Epoch [93/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [94/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [95/200], Loss: 0.0043\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [96/200], Loss: 0.0047\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [97/200], Loss: 0.0045\n",
      "Validation Loss: 0.0049, Validation Accuracy: 99.88%\n",
      "Epoch [98/200], Loss: 0.0047\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [99/200], Loss: 0.0044\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [100/200], Loss: 0.0044\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [101/200], Loss: 0.0045\n",
      "Validation Loss: 0.0052, Validation Accuracy: 99.88%\n",
      "Epoch [102/200], Loss: 0.0046\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [103/200], Loss: 0.0044\n",
      "Validation Loss: 0.0049, Validation Accuracy: 99.88%\n",
      "Epoch [104/200], Loss: 0.0043\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [105/200], Loss: 0.0049\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [106/200], Loss: 0.0048\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [107/200], Loss: 0.0046\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [108/200], Loss: 0.0050\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [109/200], Loss: 0.0045\n",
      "Validation Loss: 0.0050, Validation Accuracy: 99.88%\n",
      "Epoch [110/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [111/200], Loss: 0.0049\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [112/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [113/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [114/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [115/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [116/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [117/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [118/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [119/200], Loss: 0.0050\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [120/200], Loss: 0.0050\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [121/200], Loss: 0.0042\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [122/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [123/200], Loss: 0.0050\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [124/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [125/200], Loss: 0.0049\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [126/200], Loss: 0.0053\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [127/200], Loss: 0.0041\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [128/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [129/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [130/200], Loss: 0.0050\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [131/200], Loss: 0.0043\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [132/200], Loss: 0.0052\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [133/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [134/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [135/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [136/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [137/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [138/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [139/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [140/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [141/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [142/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [143/200], Loss: 0.0049\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [144/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [145/200], Loss: 0.0051\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [146/200], Loss: 0.0044\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [147/200], Loss: 0.0044\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [148/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [149/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [150/200], Loss: 0.0042\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [151/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [152/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [153/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [154/200], Loss: 0.0051\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [155/200], Loss: 0.0042\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [156/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [157/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [158/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [159/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [160/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [161/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [162/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [163/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [164/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [165/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [166/200], Loss: 0.0041\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [167/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [168/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [169/200], Loss: 0.0049\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [170/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [171/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [172/200], Loss: 0.0041\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [173/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [174/200], Loss: 0.0051\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [175/200], Loss: 0.0051\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [176/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [177/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [178/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [179/200], Loss: 0.0049\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [180/200], Loss: 0.0051\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [181/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [182/200], Loss: 0.0041\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [183/200], Loss: 0.0043\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [184/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [185/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [186/200], Loss: 0.0050\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [187/200], Loss: 0.0043\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [188/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [189/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [190/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [191/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [192/200], Loss: 0.0042\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [193/200], Loss: 0.0041\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [194/200], Loss: 0.0048\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [195/200], Loss: 0.0049\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [196/200], Loss: 0.0050\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [197/200], Loss: 0.0047\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [198/200], Loss: 0.0045\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [199/200], Loss: 0.0046\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n",
      "Epoch [200/200], Loss: 0.0052\n",
      "Validation Loss: 0.0051, Validation Accuracy: 99.88%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, 128),                                                                     \n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),  # Lower dropout\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),         # Consistent activation\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),\n",
    "    nn.Linear(16, output_size)  # No Softmax here\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Higher learning rate\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training step\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    correct, total, val_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save the model checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'{model_save_dir}/model_{epoch+1}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(model.state_dict(), f'{model_save_dir}/final_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10900"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rifat\\AppData\\Local\\Temp\\ipykernel_20520\\3574674988.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'{model_save_dir}/final_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAHHCAYAAACcHAM1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABp0UlEQVR4nO3deVhU5dsH8O+wDeuAuLAoIsgiKG64EbklgYZbYkqaoomloeVulgtqZj8tTc2lTEVLUyu1xFxQXFJxTRQ3UkRBWVwQRpCd8/7hy+QIjowMzmH4frzOdTHnPOc593lA5uZZzkgEQRBAREREJGJ62g6AiIiI6EWYsBAREZHoMWEhIiIi0WPCQkRERKLHhIWIiIhEjwkLERERiR4TFiIiIhI9JixEREQkekxYiIiISPSYsBBVQ9euXYO/vz8sLS0hkUiwY8cOjdZ/8+ZNSCQSREREaLTe6qxLly7o0qWLtsMgqrGYsBC9pISEBHz44YdwdnaGsbExZDIZfH19sWTJEuTm5lbptUNCQhAXF4d58+bhp59+Qps2bar0eq/SsGHDIJFIIJPJym3Ha9euQSKRQCKR4Ouvv1a7/pSUFISHhyM2NlYD0RLRq2Kg7QCIqqNdu3bhnXfegVQqxdChQ9GsWTMUFBTg6NGjmDx5Mi5duoQffvihSq6dm5uLmJgYfP755xgzZkyVXMPR0RG5ubkwNDSskvpfxMDAAI8fP8bOnTsxYMAApWMbN26EsbEx8vLyXqrulJQUzJ49G40aNULLli0rfN6+ffte6npEpBlMWIjUlJiYiODgYDg6OiI6Ohp2dnaKY2FhYbh+/Tp27dpVZde/d+8eAMDKyqrKriGRSGBsbFxl9b+IVCqFr68vfvnllzIJy6ZNmxAYGIjff//9lcTy+PFjmJqawsjI6JVcj4jKxyEhIjUtWLAA2dnZWLNmjVKyUsrFxQWffPKJ4nVRURHmzp2Lxo0bQyqVolGjRvjss8+Qn5+vdF6jRo3Qs2dPHD16FO3atYOxsTGcnZ2xYcMGRZnw8HA4OjoCACZPngyJRIJGjRoBeDKUUvr108LDwyGRSJT2RUVF4fXXX4eVlRXMzc3h7u6Ozz77THH8eXNYoqOj0bFjR5iZmcHKygp9+vTBlStXyr3e9evXMWzYMFhZWcHS0hLDhw/H48ePn9+wzxg0aBB2796NzMxMxb7Tp0/j2rVrGDRoUJnyGRkZmDRpEry8vGBubg6ZTIYePXrg/PnzijKHDh1C27ZtAQDDhw9XDC2V3meXLl3QrFkznD17Fp06dYKpqamiXZ6dwxISEgJjY+My9x8QEIBatWohJSWlwvdKRC/GhIVITTt37oSzszNee+21CpUPDQ3FzJkz0bp1ayxevBidO3fG/PnzERwcXKbs9evX0b9/f7z55pv45ptvUKtWLQwbNgyXLl0CAPTr1w+LFy8GALz77rv46aef8O2336oV/6VLl9CzZ0/k5+djzpw5+Oabb9C7d28cO3ZM5Xn79+9HQEAA7t69i/DwcEyYMAHHjx+Hr68vbt68Wab8gAED8OjRI8yfPx8DBgxAREQEZs+eXeE4+/XrB4lEgm3btin2bdq0CU2aNEHr1q3LlL9x4wZ27NiBnj17YtGiRZg8eTLi4uLQuXNnRfLg4eGBOXPmAAA++OAD/PTTT/jpp5/QqVMnRT0PHjxAjx490LJlS3z77bfo2rVrufEtWbIEdevWRUhICIqLiwEA33//Pfbt24dly5bB3t6+wvdKRBUgEFGFZWVlCQCEPn36VKh8bGysAEAIDQ1V2j9p0iQBgBAdHa3Y5+joKAAQjhw5oth39+5dQSqVChMnTlTsS0xMFAAICxcuVKozJCREcHR0LBPDrFmzhKf/qy9evFgAINy7d++5cZdeY926dYp9LVu2FOrVqyc8ePBAse/8+fOCnp6eMHTo0DLXe//995XqfPvtt4XatWs/95pP34eZmZkgCILQv39/oVu3boIgCEJxcbFga2srzJ49u9w2yMvLE4qLi8vch1QqFebMmaPYd/r06TL3Vqpz584CAGHVqlXlHuvcubPSvr179woAhC+++EK4ceOGYG5uLvTt2/eF90hE6mMPC5Ea5HI5AMDCwqJC5f/66y8AwIQJE5T2T5w4EQDKzHXx9PREx44dFa/r1q0Ld3d33Lhx46Vjflbp3Jc//vgDJSUlFTonNTUVsbGxGDZsGKytrRX7mzdvjjfffFNxn08bNWqU0uuOHTviwYMHijasiEGDBuHQoUNIS0tDdHQ00tLSyh0OAp7Me9HTe/Irrbi4GA8ePFAMd/3zzz8VvqZUKsXw4cMrVNbf3x8ffvgh5syZg379+sHY2Bjff/99ha9FRBXHhIVIDTKZDADw6NGjCpW/desW9PT04OLiorTf1tYWVlZWuHXrltL+hg0blqmjVq1aePjw4UtGXNbAgQPh6+uL0NBQ2NjYIDg4GFu3blWZvJTG6e7uXuaYh4cH7t+/j5ycHKX9z95LrVq1AECte3nrrbdgYWGBLVu2YOPGjWjbtm2ZtixVUlKCxYsXw9XVFVKpFHXq1EHdunVx4cIFZGVlVfia9evXV2uC7ddffw1ra2vExsZi6dKlqFevXoXPJaKKY8JCpAaZTAZ7e3tcvHhRrfOenfT6PPr6+uXuFwThpa9ROr+ilImJCY4cOYL9+/djyJAhuHDhAgYOHIg333yzTNnKqMy9lJJKpejXrx/Wr1+P7du3P7d3BQC+/PJLTJgwAZ06dcLPP/+MvXv3IioqCk2bNq1wTxLwpH3Uce7cOdy9excAEBcXp9a5RFRxTFiI1NSzZ08kJCQgJibmhWUdHR1RUlKCa9euKe1PT09HZmamYsWPJtSqVUtpRU2pZ3txAEBPTw/dunXDokWLcPnyZcybNw/R0dE4ePBguXWXxhkfH1/m2NWrV1GnTh2YmZlV7gaeY9CgQTh37hwePXpU7kTlUr/99hu6du2KNWvWIDg4GP7+/vDz8yvTJhVNHisiJycHw4cPh6enJz744AMsWLAAp0+f1lj9RPQfJixEapoyZQrMzMwQGhqK9PT0MscTEhKwZMkSAE+GNACUWcmzaNEiAEBgYKDG4mrcuDGysrJw4cIFxb7U1FRs375dqVxGRkaZc0sfoPbsUutSdnZ2aNmyJdavX6+UAFy8eBH79u1T3GdV6Nq1K+bOnYvvvvsOtra2zy2nr69fpvfm119/xZ07d5T2lSZW5SV36po6dSqSkpKwfv16LFq0CI0aNUJISMhz25GIXh4fHEekpsaNG2PTpk0YOHAgPDw8lJ50e/z4cfz6668YNmwYAKBFixYICQnBDz/8gMzMTHTu3BmnTp3C+vXr0bdv3+cumX0ZwcHBmDp1Kt5++218/PHHePz4MVauXAk3NzelSadz5szBkSNHEBgYCEdHR9y9excrVqxAgwYN8Prrrz+3/oULF6JHjx7w8fHBiBEjkJubi2XLlsHS0hLh4eEau49n6enpYfr06S8s17NnT8yZMwfDhw/Ha6+9hri4OGzcuBHOzs5K5Ro3bgwrKyusWrUKFhYWMDMzQ/v27eHk5KRWXNHR0VixYgVmzZqlWGa9bt06dOnSBTNmzMCCBQvUqo+IXkDLq5SIqq1///1XGDlypNCoUSPByMhIsLCwEHx9fYVly5YJeXl5inKFhYXC7NmzBScnJ8HQ0FBwcHAQpk2bplRGEJ4saw4MDCxznWeX0z5vWbMgCMK+ffuEZs2aCUZGRoK7u7vw888/l1nWfODAAaFPnz6Cvb29YGRkJNjb2wvvvvuu8O+//5a5xrNLf/fv3y/4+voKJiYmgkwmE3r16iVcvnxZqUzp9Z5dNr1u3ToBgJCYmPjcNhUE5WXNz/O8Zc0TJ04U7OzsBBMTE8HX11eIiYkpdznyH3/8IXh6egoGBgZK99m5c2ehadOm5V7z6Xrkcrng6OgotG7dWigsLFQqN378eEFPT0+IiYlReQ9EpB6JIKgxA46IiIhICziHhYiIiESPCQsRERGJHhMWIiIiEj0mLERERCR6TFiIiIhI9JiwEBERkejxwXFVrKSkBCkpKbCwsNDoI8GJiOjVEAQBjx49gr29veITwTUtLy8PBQUFGqnLyMgIxsbGGqlLTJiwVLGUlBQ4ODhoOwwiIqqk5ORkNGjQQOP15uXlwcSiNlD0WCP12draIjExUeeSFiYsVczCwgIAYOQZAol+xT+ynl5e0qGvtR0CEemQR3I5XJwcFL/PNa2goAAoegypZwhQ2feJ4gKkXV6PgoICJiykntJhIIm+EROWV0Qmk2k7BCLSQVU+rG9gXOn3CUGiu1NTmbAQERGJgQRAZZMiHZ4qyYSFiIhIDCR6T7bK1qGjdPfOiIiISGewh4WIiEgMJBINDAnp7pgQExYiIiIx4JCQSrp7Z0RERKQz2MNCREQkBhwSUokJCxERkShoYEhIhwdOdPfOiIiISGewh4WIiEgMOCSkEhMWIiIiMeAqIZV0986IiIhIZ7CHhYiISAw4JKQSExYiIiIx4JCQSkxYiIiIxIA9LCrpbipGREREOoM9LERERGLAISGVmLAQERGJgUSigYSFQ0JEREREWsMeFiIiIjHQkzzZKluHjmLCQkREJAacw6KS7t4ZERER6Qz2sBAREYkBn8OiEhMWIiIiMeCQkEq6e2dERESkM9jDQkREJAYcElKJCQsREZEYcEhIJSYsREREYsAeFpV0NxUjIiIincEeFiIiIjHgkJBKTFiIiIjEgENCKuluKkZEREQ6gz0sREREoqCBISEd7odgwkJERCQGHBJSSXdTMSIiItIZ7GEhIiISA4lEA6uEdLeHhQkLERGRGHBZs0q6e2dERESk0sqVK9G8eXPIZDLIZDL4+Phg9+7diuNdunSBRCJR2kaNGqVUR1JSEgIDA2Fqaop69eph8uTJKCoqUipz6NAhtG7dGlKpFC4uLoiIiFA71mrRwyKRSLB9+3b07dtX26FUK+8HvY73gzrCwc4aAHD1RhoWrtmN/ccvAwAa1a+DuZ+8jQ4tnWFkaIADMVcw9etfcS/jEQDAt7UrIr//pNy63whZgHOXk+DiWA+LPg2Gu5MtZOYmSLufhd/2nMH/Vv+FouKSV3Oj1Vjz3jORnJpRZv+I/h3x9dSBWoio5li99TCW/XwAdx/I0cy1Pv43+R14N22k7bB0Ftu7ArQw6bZBgwb46quv4OrqCkEQsH79evTp0wfnzp1D06ZNAQAjR47EnDlzFOeYmpoqvi4uLkZgYCBsbW1x/PhxpKamYujQoTA0NMSXX34JAEhMTERgYCBGjRqFjRs34sCBAwgNDYWdnR0CAgIqHKvWE5a0tDTMmzcPu3btwp07d1CvXj20bNkS48aNQ7du3bQdHgRBwKxZs7B69WpkZmbC19cXK1euhKurq7ZDe6GUu5mY/d0fSEi+B4lEgncD22Pj1x+g83tfISklA9u+C8PFa3fQZ/QyAMBnowLxy6IP8ebwbyAIAk5duAH37tOU6vxsVE90buuOc5eTAACFRcXY/NcpXLiajKxHj9HMrQG+/exd6OlJMHfFzld+z9VN9PrJKC4WFK+vJKTg7THfoa9fKy1Gpfu27TuL6d9ux6JPB8K7WSOs+uUggsYux+nfZqKutYW2w9M5bO8K0sKQUK9evZRez5s3DytXrsSJEycUCYupqSlsbW3LPX/fvn24fPky9u/fDxsbG7Rs2RJz587F1KlTER4eDiMjI6xatQpOTk745ptvAAAeHh44evQoFi9erFbCotUhoZs3b8Lb2xvR0dFYuHAh4uLisGfPHnTt2hVhYWHaDE1hwYIFWLp0KVatWoWTJ0/CzMwMAQEByMvL03ZoL7Tn74uIOn4ZN5LvISHpLr5YuRM5j/PRppkT2rdwRkO72gib/TMuJ6TgckIKPgr/Ca08GqJTWzcAT5KRuw8eKbaMzBy81ak5Nu48objGrTsPsGnnCVy8dgfJaQ+x+0gcft1zBj4tG2vrtquVOrUsYFNHptj2Hr0IpwZ14Nta/AlxdbZiUzSG9n0Ng3v7oImzHRZNC4apsRF+/jNG26HpJLZ3BZX2sFR2AyCXy5W2/Pz8F16+uLgYmzdvRk5ODnx8fBT7N27ciDp16qBZs2aYNm0aHj9+rDgWExMDLy8v2NjYKPYFBARALpfj0qVLijJ+fn5K1woICEBMjHrff60mLB999BEkEglOnTqFoKAguLm5oWnTppgwYQJOnDjx3POmTp0KNzc3mJqawtnZGTNmzEBhYaHi+Pnz59G1a1dYWFhAJpPB29sbZ86cAQDcunULvXr1Qq1atWBmZoamTZvir7/+Kvc6giDg22+/xfTp09GnTx80b94cGzZsQEpKCnbs2KHRtqhqenoS9HvTG6YmRjgdlwipkQEEQUB+wX/jjHkFRSgpEdChRfnJRo9OzWFtaYZNO5//vXFqUAfdfDxw7J/rGr8HXVdQWIStu09jcG8fSHR4pr+2FRQWIfZqMrq0c1fs09PTQ+d27jgdl6jFyHQT21s7HBwcYGlpqdjmz5//3LJxcXEwNzeHVCrFqFGjsH37dnh6egIABg0ahJ9//hkHDx7EtGnT8NNPP+G9995TnJuWlqaUrABQvE5LS1NZRi6XIzc3t8L3pLUhoYyMDOzZswfz5s2DmZlZmeNWVlbPPdfCwgIRERGwt7dHXFwcRo4cCQsLC0yZMgUAMHjwYLRq1QorV66Evr4+YmNjYWhoCAAICwtDQUEBjhw5AjMzM1y+fBnm5ublXicxMRFpaWlKmaGlpSXat2+PmJgYBAcHlzknPz9fKZOVy+UVao+q4tnYHnvXToSxkQFycvMxZPJqxCem4f7DbDzOK0D42D6Yu/xPSCQSzBrTBwYG+rCtIyu3riF9fBB94gpS7maWObZ3zQQ0d3eAsdQQEduO4svvd1XxnemeXYcuICs7F4N6ttd2KDrtQWY2iotLygxF1LWW4drNdC1FpbvY3mrQ4JBQcnIyZLL/fpdLpdLnnuLu7o7Y2FhkZWXht99+Q0hICA4fPgxPT0988MEHinJeXl6ws7NDt27dkJCQgMaNX21PutYSluvXr0MQBDRp0kTtc6dPn674ulGjRpg0aRI2b96sSFiSkpIwefJkRd1PzzdJSkpCUFAQvLy8AADOzs7PvU5pdlheZlh67Fnz58/H7Nmz1b6nqnLtVjo6DZ4PmbkJ+nRrhRXhQ9DzwyWIT0zDsE/X4JtPB+LDgZ1RUiLg931nEXslCSUlQpl67OtZ4Y0OHhg+bW2513n/s7UwNzVGM9f6mP1xX4x9rxuW/rS/qm9Pp/z853H4+XjCrq6VtkMhIm3Q4KTb0lU/FWFkZAQXFxcAgLe3N06fPo0lS5bg+++/L1O2ffsnf1Bdv34djRs3hq2tLU6dOqVUJj39SSJaOu/F1tZWse/pMjKZDCYmJhW+Na0NCQlC2TfFitqyZQt8fX1ha2sLc3NzTJ8+HUlJSYrjEyZMQGhoKPz8/PDVV18hISFBcezjjz/GF198AV9fX8yaNQsXLlyo1H08a9q0acjKylJsycnJGq1fXYVFxUi8fR/nryZjzvI/cfHaHYwK7gIAOHjyKlq/PRuu/tPQ+M1PMWrWBtjVs8LNO/fL1DOoVwdkZOVg95Hy2+tOeibiE9Pw+76zmLP8T0z94C3o6XFYo6KSUjNw6FQ8hvZ9Tduh6LzaVubQ19dTrIYrdS9Djnq1K/YLniqO7V39lJSUPHfOS2xsLADAzs4OAODj44O4uDjcvXtXUSYqKgoymUwxrOTj44MDBw4o1RMVFaU0T6YitJawuLq6QiKR4OrVq2qdFxMTg8GDB+Ott95CZGQkzp07h88//xwFBQWKMuHh4bh06RICAwMRHR0NT09PbN++HQAQGhqKGzduYMiQIYiLi0ObNm2wbNmycq9Vmh2Wlxk+b8a0VCpVZLbqZLivip5EAiMj5Y61jKwcyLNz0bGNG+rWMsfuv+PKnDe4Vwds/utUhZYqSyQSGBroQ4/zMCps084Y1K1lAX/fptoORecZGRqgZRMHHD4dr9hXUlKCI6f/RVsvJy1GppvY3hX37PNOXnZTx7Rp03DkyBHcvHkTcXFxmDZtGg4dOoTBgwcjISEBc+fOxdmzZ3Hz5k38+eefGDp0KDp16oTmzZsDAPz9/eHp6YkhQ4bg/Pnz2Lt3L6ZPn46wsDDFMNSoUaNw48YNTJkyBVevXsWKFSuwdetWjB8/Xq1YtZawWFtbIyAgAMuXL0dOTk6Z45mZmeWed/z4cTg6OuLzzz9HmzZt4Orqilu3bpUp5+bmhvHjx2Pfvn3o168f1q1bpzjm4OCAUaNGYdu2bZg4cSJWr15d7rWcnJxga2urlBnK5XKcPHlS7cxQG2aG9cZrrRrDwc4ano3tMTOsN173dsWvu59MQB7UqwPaNGuERvXrYECPtoiYPwIrfjmI67fuKtXTqa0bGtWvg592HC9zjXe6t0Ffv1Zwa2QDx/q10devFWaG9cb2qLN8DksFlZSUYOPOEwgObA8DA31th1MjfDToDWzYcRy/RJ5AfGIaJny1BTm5+Rjcq4O2Q9NJbO+K0UbCcvfuXQwdOhTu7u7o1q0bTp8+jb179+LNN9+EkZER9u/fD39/fzRp0gQTJ05EUFAQdu7875EV+vr6iIyMhL6+Pnx8fPDee+9h6NChSs9tcXJywq5duxAVFYUWLVrgm2++wY8//qjWkmZAy89hWb58OXx9fdGuXTvMmTMHzZs3R1FREaKiorBy5UpcuXKlzDmurq5ISkrC5s2b0bZtW+zatUvRewIAubm5mDx5Mvr37w8nJyfcvn0bp0+fRlBQEABg3Lhx6NGjB9zc3PDw4UMcPHgQHh4e5cYnkUgwbtw4fPHFF3B1dYWTkxNmzJgBe3v7avEQuzq1zLEyfChs6sggz87Dpet3EDR2BQ6detKr5epYDzPDeqOWzBRJKRn4Zt1erNgUXaaeIb1fw8nzCbh2q+wEuaLiEnwy9E00blgPEokEyWkZ+PHXI+XWQ+U7dCoet9Me4r3e/OX9qvTz98b9zGx8+f0u3H3wCF5u9fHb0jAOUVQRtrd4rVmz5rnHHBwccPjw4RfW4ejo+NzVtqW6dOmCc+fOqR3f0yRCZSaTaEBqairmzZuHyMhIpKamom7duvD29sb48ePRpUuXJ0E+86TbKVOmYO3atcjPz0dgYCA6dOiA8PBwZGZmoqCgACEhITh27BjS09NRp04d9OvXDwsXLoSxsTHGjh2L3bt34/bt25DJZOjevTsWL16M2rVrlxtf6YPjfvjhB2RmZuL111/HihUr4ObmVqH7k8vlsLS0hNRrJCT6RppoMnqBh6e/03YIRKRD5HI5bGpbIisrq0qG+UvfJ0z6LIfEsOKTUMsjFOYi94+wKotVm7SesOg6JiyvHhMWItKkV5WwmPZdoZGE5fGOj3QyYeGHHxIREZHoaf2zhIiIiAgvNWm2nEo0E4wIMWEhIiISASYsqjFhISIiEgEmLKpxDgsRERGJHntYiIiIxEDy/1tl69BRTFiIiIhEgENCqnFIiIiIiESPPSxEREQiIJFAAz0smolFjJiwEBERiYAEGhgS0uGMhUNCREREJHrsYSEiIhIBTrpVjQkLERGRGHBZs0ocEiIiIiLRYw8LERGRGGhgSEjgkBARERFVJU3MYan8KiPxYsJCREQkAkxYVOMcFiIiIhI99rAQERGJAVcJqcSEhYiISAQ4JKQah4SIiIhI9NjDQkREJALsYVGNCQsREZEIMGFRjUNCREREJHrsYSEiIhIB9rCoxoSFiIhIDLisWSUOCREREZHosYeFiIhIBDgkpBoTFiIiIhFgwqIaExYiIiIRYMKiGuewEBERkeixh4WIiEgMuEpIJSYsREREIsAhIdU4JERERFRDrVy5Es2bN4dMJoNMJoOPjw92796tOJ6Xl4ewsDDUrl0b5ubmCAoKQnp6ulIdSUlJCAwMhKmpKerVq4fJkyejqKhIqcyhQ4fQunVrSKVSuLi4ICIiQu1YmbAQERGJQGkPS2U3dTRo0ABfffUVzp49izNnzuCNN95Anz59cOnSJQDA+PHjsXPnTvz66684fPgwUlJS0K9fP8X5xcXFCAwMREFBAY4fP47169cjIiICM2fOVJRJTExEYGAgunbtitjYWIwbNw6hoaHYu3eveu0jCIKg1hmkFrlcDktLS0i9RkKib6TtcGqEh6e/03YIRKRD5HI5bGpbIisrCzKZrErqt7S0hMOHW6AnNa1UXSX5j5H8/cBKxWptbY2FCxeif//+qFu3LjZt2oT+/fsDAK5evQoPDw/ExMSgQ4cO2L17N3r27ImUlBTY2NgAAFatWoWpU6fi3r17MDIywtSpU7Fr1y5cvHhRcY3g4GBkZmZiz549FY6LPSxERESE4uJibN68GTk5OfDx8cHZs2dRWFgIPz8/RZkmTZqgYcOGiImJAQDExMTAy8tLkawAQEBAAORyuaKXJiYmRqmO0jKldVQUJ90SERGJgCYn3crlcqX9UqkUUqm03HPi4uLg4+ODvLw8mJubY/v27fD09ERsbCyMjIxgZWWlVN7GxgZpaWkAgLS0NKVkpfR46TFVZeRyOXJzc2FiYlKhe2MPCxERkRhINLQBcHBwgKWlpWKbP3/+cy/r7u6O2NhYnDx5EqNHj0ZISAguX75cNfdYCexheUWSDn1dJWOfVNbaUze1HUKN8367RtoOgYiekpycrPSe87zeFQAwMjKCi4sLAMDb2xunT5/GkiVLMHDgQBQUFCAzM1OplyU9PR22trYAAFtbW5w6dUqpvtJVRE+XeXZlUXp6OmQyWYV7VwD2sBAREYmCJlcJlS5TLt1UJSzPKikpQX5+Pry9vWFoaIgDBw4ojsXHxyMpKQk+Pj4AAB8fH8TFxeHu3buKMlFRUZDJZPD09FSUebqO0jKldVQUe1iIiIhEQBsPjps2bRp69OiBhg0b4tGjR9i0aRMOHTqEvXv3wtLSEiNGjMCECRNgbW0NmUyGsWPHwsfHBx06dAAA+Pv7w9PTE0OGDMGCBQuQlpaG6dOnIywsTJEkjRo1Ct999x2mTJmC999/H9HR0di6dSt27dqlVqxMWIiIiERAInmyVbYOddy9exdDhw5FamoqLC0t0bx5c+zduxdvvvkmAGDx4sXQ09NDUFAQ8vPzERAQgBUrVijO19fXR2RkJEaPHg0fHx+YmZkhJCQEc+bMUZRxcnLCrl27MH78eCxZsgQNGjTAjz/+iICAAPXujc9hqVql6+vTH1TN+n0qi3NYXj3OYSFd9qqew+I05jeNPIcl8bv+VRarNrGHhYiISASe9LBUdkhIQ8GIEBMWIiIiMdDAkJAuf1ozVwkRERGR6LGHhYiISAS0sUqoOmHCQkREJALaWCVUnXBIiIiIiESPPSxEREQioKcngZ5e5bpIhEqeL2ZMWIiIiESAQ0KqcUiIiIiIRI89LERERCLAVUKqMWEhIiISAQ4JqcaEhYiISATYw6Ia57AQERGR6LGHhYiISATYw6IaExYiIiIR4BwW1TgkRERERKLHHhYiIiIRkEADQ0LQ3S4WJixEREQiwCEh1TgkRERERKLHHhYiIiIR4Coh1ZiwEBERiQCHhFTjkBARERGJHntYiIiIRIBDQqoxYSEiIhIBDgmpxoSFiIhIBNjDohrnsBAREZHosYeFiIhIDDQwJKTDD7plwkJERCQGHBJSjUNCREREJHrsYSEiIhIBrhJSjQkLERGRCHBISDUOCREREZHosYeFiIhIBDgkpBoTFiIiIhHgkJBqHBIiIiIi0WMPCxERkQiwh0W1apGwSCQSbN++HX379tV2KDpv0bq9iDx4HtdupcNYaoh2zZ0RPqYPXBvZaDu0aiHhWjIO7T+N28npkGflYNgHfeDVwlVx/JcNu3Hm5CWlc9w9GuGDMf0BANf/TcLKJVvLrfuTKYPR0NEOAJBy5x62bdmP5FtpMDM3xetdWuGNN9tV0V3pptVbD2PZzwdw94EczVzr43+T34F300baDktnsb1fTBtzWObPn49t27bh6tWrMDExwWuvvYb//e9/cHd3V5Tp0qULDh8+rHTehx9+iFWrVileJyUlYfTo0Th48CDMzc0REhKC+fPnw8DgvzTj0KFDmDBhAi5dugQHBwdMnz4dw4YNq3CsWk9Y0tLSMG/ePOzatQt37txBvXr10LJlS4wbNw7dunXTdnjYtm0bVq1ahbNnzyIjIwPnzp1Dy5YttR1WlTn+z3WEvtMJrTwdUVRcjLkrdqLf2O9wYut0mJlItR2e6BUUFMK+QT208/FCxOo/yi3TxLMRBr7XQ/HawFBf8XUj5/qY9eVopfJ7Io/iWnwSHBraAgDycvPxw7Jf4drEEf2D30Rqyn1s+XkPTEyk8Hm9RRXcle7Ztu8spn+7HYs+HQjvZo2w6peDCBq7HKd/m4m61hbaDk/nsL0rRhs9LIcPH0ZYWBjatm2LoqIifPbZZ/D398fly5dhZmamKDdy5EjMmTNH8drU1FTxdXFxMQIDA2Fra4vjx48jNTUVQ4cOhaGhIb788ksAQGJiIgIDAzFq1Chs3LgRBw4cQGhoKOzs7BAQEFChWLWasNy8eRO+vr6wsrLCwoUL4eXlhcLCQuzduxdhYWG4evWqNsMDAOTk5OD111/HgAEDMHLkSG2HU+V+Wxam9HrFrPfg6j8NsVeS4dvaRUtRVR8eTZ3h0dRZZRl9AwPILM3KPWZgoK90rLi4GJcuXMfrXVorfhH9c/oKiopLMPC97jAw0IetfR3cuX0Xh6PPMmGpoBWbojG072sY3NsHALBoWjD2HbuEn/+Mwfhh/lqOTvewvcVrz549Sq8jIiJQr149nD17Fp06dVLsNzU1ha2tbbl17Nu3D5cvX8b+/fthY2ODli1bYu7cuZg6dSrCw8NhZGSEVatWwcnJCd988w0AwMPDA0ePHsXixYsrnLBoddLtRx99BIlEglOnTiEoKAhubm5o2rQpJkyYgBMnTjz3vKlTp8LNzQ2mpqZwdnbGjBkzUFhYqDh+/vx5dO3aFRYWFpDJZPD29saZM2cAALdu3UKvXr1Qq1YtmJmZoWnTpvjrr7+ee60hQ4Zg5syZ8PPz09yNVyPy7DwAQC2Z6QtKUkUlXEvGrKnL8dXsNfjtlyjkZOc+t+ylCwnIyclD2w7NFPtuJqbA2aUBDAz+65lp4tkI99Iz8PhxXpXGrgsKCosQezUZXdr91+Wtp6eHzu3ccTouUYuR6Sa2d8WVDglVdgMAuVyutOXn51cohqysLACAtbW10v6NGzeiTp06aNasGaZNm4bHjx8rjsXExMDLyws2Nv9NHQgICIBcLselS5cUZZ59Hw0ICEBMTEyF20drPSwZGRnYs2cP5s2bp9TtVMrKyuq551pYWCAiIgL29vaIi4vDyJEjYWFhgSlTpgAABg8ejFatWmHlypXQ19dHbGwsDA0NAQBhYWEoKCjAkSNHYGZmhsuXL8Pc3LxK7rG6KykpwbRFv6F9C2d4uthrOxyd0MTTCV4tXVG7tiXu38/E7j//xuoVv+PjSYOgp1f274eTx+Pg7tEIVrX+6zZ/JM+BdW1LpXLmFqaKY6amxlV7E9Xcg8xsFBeXlBmKqGstw7Wb6VqKSnexvStOk0NCDg4OSvtnzZqF8PBwleeWlJRg3Lhx8PX1RbNm//2RNGjQIDg6OsLe3h4XLlzA1KlTER8fj23btgF4MrXj6WQFgOJ1WlqayjJyuRy5ubkwMTF54b1pLWG5fv06BEFAkyZN1D53+vTpiq8bNWqESZMmYfPmzYqEJSkpCZMnT1bU7er636THpKQkBAUFwcvLCwDg7Ky6+15d+fn5SpmsXC7XaP2v0qQFW3ElIRW7V4/Xdig6o1Wb/37e7erXhX39uvhy1o+4/m8y3Jo4KpXNfPgI8VduYuiIXq86TCKq5pKTkyGTyRSvpdIXz0EMCwvDxYsXcfToUaX9H3zwgeJrLy8v2NnZoVu3bkhISEDjxo01F/QLaG1ISBCElz53y5Yt8PX1ha2tLczNzTF9+nQkJSUpjk+YMAGhoaHw8/PDV199hYSEBMWxjz/+GF988QV8fX0xa9YsXLhwoVL38az58+fD0tJSsT2b5VYXkxdsxd6/L2Lnyo9R36aWtsPRWbXrWMHM3AQP7mWWOXb6xEWYmRmjaXPlXwgWMjM8evRYaV/2/7+2kJU/N4b+U9vKHPr6eriX8Uhp/70MOerVlj3nLHpZbO+Kk0ADQ0L/X5dMJlPaXpSwjBkzBpGRkTh48CAaNGigsmz79u0BPOl4AABbW1ukpyv3lpW+Lp338rwyMpmsQr0rgBYTFldXV0gkErUn1sbExGDw4MF46623EBkZiXPnzuHzzz9HQUGBokx4eDguXbqEwMBAREdHw9PTE9u3bwcAhIaG4saNGxgyZAji4uLQpk0bLFu2TGP3NW3aNGRlZSm25ORkjdX9KgiCgMkLtmLXofP4c+XHcKxfR9sh6bTMh4/wOCcXFs9MwhUEAadiLsK7fVPo6+srHWvkZI8b12+juLhYse/fK7dQ18aaw0EVYGRogJZNHHD4dLxiX0lJCY6c/hdtvZy0GJluYntXnJ5EopFNHYIgYMyYMdi+fTuio6Ph5PTi70lsbCwAwM7uyWMWfHx8EBcXh7t37yrKREVFQSaTwdPTU1HmwIEDSvVERUXBx8enwrFqLWGxtrZGQEAAli9fjpycnDLHMzMzyz3v+PHjcHR0xOeff442bdrA1dUVt27dKlPOzc0N48ePx759+9CvXz+sW7dOcczBwQGjRo3Ctm3bMHHiRKxevVpj9yWVSstkttXJpP9txdbdp7F67jCYmxoj/b4c6fflyM0rePHJhPy8AtxJvos7yU/+42Y8yMKd5Lt4mCFHfl4Bdm47hFuJKch4kIV/r97Cuu93oHbdWmji0UipnmvxSch4kIX2r3mVuUarth4w0NfDlp/3Ii3lPs6dvYq/D51F5ze8X8Ut6oSPBr2BDTuO45fIE4hPTMOEr7YgJzcfg3t10HZoOontLV5hYWH4+eefsWnTJlhYWCAtLQ1paWnIzX2yGCAhIQFz587F2bNncfPmTfz5558YOnQoOnXqhObNmwMA/P394enpiSFDhuD8+fPYu3cvpk+fjrCwMEXPzqhRo3Djxg1MmTIFV69exYoVK7B161aMH1/xKQdaXda8fPly+Pr6ol27dpgzZw6aN2+OoqIiREVFYeXKlbhy5UqZc1xdXZGUlITNmzejbdu22LVrl6L3BAByc3MxefJk9O/fH05OTrh9+zZOnz6NoKAgAMC4cePQo0cPuLm54eHDhzh48CA8PDyeG2NGRgaSkpKQkpICAIiPf/JXgq2t7XOXeFVna3//GwDQc9QSpf3LZ76HQfzl8kLJSWlKD3778/dDAIA27Zuif7AfUlLu48zJS8jNzYfM0hzuHo3QvacvDAyV/yueiolDI2d72NjWLnMNExMpPhj7DrZt2Y/F//sJZuYmeLOHD5c0q6GfvzfuZ2bjy+934e6DR/Byq4/floZxiKKKsL0rRhsPjlu5ciWAJw+He9q6deswbNgwGBkZYf/+/fj222+Rk5MDBwcHBAUFKc0l1dfXR2RkJEaPHg0fHx+YmZkhJCRE6bktTk5O2LVrF8aPH48lS5agQYMG+PHHHyu8pBkAJEJlJpNoQGpqKubNm4fIyEikpqaibt268Pb2xvjx4xUN+OyTbqdMmYK1a9ciPz8fgYGB6NChA8LDw5GZmYmCggKEhITg2LFjSE9PR506ddCvXz8sXLgQxsbGGDt2LHbv3o3bt29DJpOhe/fuWLx4MWrXLvvGADxZkz58+PAy+ysy4xp4MunW0tIS6Q+yql1vS3W19tRNbYdQ47zfrpG2QyCqMnK5HDa1LZGVVTW/x0vfJ974+gAMTCo3D60oNwfRk7pVWazapPWERdcxYXn1mLC8ekxYSJe9qoTF7xvNJCz7J+pmwsJPayYiIiLR0/pnCREREREAiQY+bVl3P6yZCQsREZEYaGPSbXXCISEiIiISPfawEBERiYDk//9Vtg5dxYSFiIhIBPQkT7bK1qGrOCREREREosceFiIiIhGQSCSVXiVU6VVGIlahhOXPP/+scIW9e/d+6WCIiIhqKq4SUq1CCUvpI/FfRCKRKH2CLBEREZEmVChhKSkpqeo4iIiIajQ9iQR6lewiqez5YlapOSx5eXkwNjbWVCxEREQ1FoeEVFN7lVBxcTHmzp2L+vXrw9zcHDdu3AAAzJgxA2vWrNF4gERERDVB6aTbym66Su2EZd68eYiIiMCCBQtgZGSk2N+sWTP8+OOPGg2OiIiICHiJhGXDhg344YcfMHjwYOjr6yv2t2jRAlevXtVocERERDVF6ZBQZTddpfYcljt37sDFxaXM/pKSEhQWFmokKCIiopqGk25VU7uHxdPTE3///XeZ/b/99htatWqlkaCIiIiInqZ2D8vMmTMREhKCO3fuoKSkBNu2bUN8fDw2bNiAyMjIqoiRiIhI50n+f6tsHbpK7R6WPn36YOfOndi/fz/MzMwwc+ZMXLlyBTt37sSbb75ZFTESERHpPK4SUu2lnsPSsWNHREVFaToWIiIionK99IPjzpw5gytXrgB4Mq/F29tbY0ERERHVNHqSJ1tl69BVaicst2/fxrvvvotjx47BysoKAJCZmYnXXnsNmzdvRoMGDTQdIxERkc7jpzWrpvYcltDQUBQWFuLKlSvIyMhARkYGrly5gpKSEoSGhlZFjERERFTDqd3DcvjwYRw/fhzu7u6Kfe7u7li2bBk6duyo0eCIiIhqEh3uIKk0tRMWBweHch8QV1xcDHt7e40ERUREVNNwSEg1tYeEFi5ciLFjx+LMmTOKfWfOnMEnn3yCr7/+WqPBERER1RSlk24ru+mqCvWw1KpVSylry8nJQfv27WFg8OT0oqIiGBgY4P3330ffvn2rJFAiIiKquSqUsHz77bdVHAYREVHNxiEh1SqUsISEhFR1HERERDUaH82v2ks/OA4A8vLyUFBQoLRPJpNVKiAiIiKiZ6mdsOTk5GDq1KnYunUrHjx4UOZ4cXGxRgIjIiKqSfQkEuhVckinsueLmdqrhKZMmYLo6GisXLkSUqkUP/74I2bPng17e3ts2LChKmIkIiLSeRKJZjZdpXYPy86dO7FhwwZ06dIFw4cPR8eOHeHi4gJHR0ds3LgRgwcProo4iYiIqAZTu4clIyMDzs7OAJ7MV8nIyAAAvP766zhy5IhmoyMiIqohSlcJVXbTVWonLM7OzkhMTAQANGnSBFu3bgXwpOel9MMQiYiISD3aGBKaP38+2rZtCwsLC9SrVw99+/ZFfHy8Upm8vDyEhYWhdu3aMDc3R1BQENLT05XKJCUlITAwEKampqhXrx4mT56MoqIipTKHDh1C69atIZVK4eLigoiICLViVTthGT58OM6fPw8A+PTTT7F8+XIYGxtj/PjxmDx5srrVERERkZYcPnwYYWFhOHHiBKKiolBYWAh/f3/k5OQoyowfPx47d+7Er7/+isOHDyMlJQX9+vVTHC8uLkZgYCAKCgpw/PhxrF+/HhEREZg5c6aiTGJiIgIDA9G1a1fExsZi3LhxCA0Nxd69eyscq0QQBKEyN3vr1i2cPXsWLi4uaN68eWWq0klyuRyWlpZIf5DFJd+vyNpTN7UdQo3zfrtG2g6BqMrI5XLY1LZEVlbV/B4vfZ94f8NJGJmaV6qugsfZWDu0/UvHeu/ePdSrVw+HDx9Gp06dkJWVhbp162LTpk3o378/AODq1avw8PBATEwMOnTogN27d6Nnz55ISUmBjY0NAGDVqlWYOnUq7t27ByMjI0ydOhW7du3CxYsXFdcKDg5GZmYm9uzZU6HY1O5heZajoyP69evHZIWIiKgSxLBKKCsrCwBgbW0NADh79iwKCwvh5+enKNOkSRM0bNgQMTExAICYmBh4eXkpkhUACAgIgFwux6VLlxRlnq6jtExpHRVRoVVCS5curXCFH3/8cYXLEhER0ROafDS/XC5X2i+VSiGVSlWeW1JSgnHjxsHX1xfNmjUDAKSlpcHIyKjMHFUbGxukpaUpyjydrJQeLz2mqoxcLkdubi5MTExeeG8VSlgWL15ckWKQSCRMWIiIiLTMwcFB6fWsWbMQHh6u8pywsDBcvHgRR48ercLIXl6FEpbSVUFE1QHnU7x6529lajuEGqWFo5W2Q6AqoIfKz9MoPT85OVlpDsuLelfGjBmDyMhIHDlyBA0aNFDst7W1RUFBATIzM5V6WdLT02Fra6soc+rUKaX6SlcRPV3m2ZVF6enpkMlkFepdefreiIiISIs0+RwWmUymtD0vYREEAWPGjMH27dsRHR0NJycnpePe3t4wNDTEgQMHFPvi4+ORlJQEHx8fAICPjw/i4uJw9+5dRZmoqCjIZDJ4enoqyjxdR2mZ0joqolIffkhERETVV1hYGDZt2oQ//vgDFhYWijknlpaWMDExgaWlJUaMGIEJEybA2toaMpkMY8eOhY+PDzp06AAA8Pf3h6enJ4YMGYIFCxYgLS0N06dPR1hYmCJRGjVqFL777jtMmTIF77//PqKjo7F161bs2rWrwrEyYSEiIhIBiQTQq+QqH3Xn7K5cuRIA0KVLF6X969atw7BhwwA8mceqp6eHoKAg5OfnIyAgACtWrFCU1dfXR2RkJEaPHg0fHx+YmZkhJCQEc+bMUZRxcnLCrl27MH78eCxZsgQNGjTAjz/+iICAgIrfW2Wfw0Kq8TksVBNwDsurxTksr9areg7LR7+chrSSz2HJf5yNFe+2rbJYtYlzWIiIiEj0Xiph+fvvv/Hee+/Bx8cHd+7cAQD89NNPol0KRUREJHb88EPV1E5Yfv/9dwQEBMDExATnzp1Dfn4+gCdPx/vyyy81HiAREVFNoCfRzKar1E5YvvjiC6xatQqrV6+GoaGhYr+vry/++ecfjQZHREREBLzEKqH4+Hh06tSpzH5LS0tkZmZqIiYiIqIaRxOfBaTDI0Lq97DY2tri+vXrZfYfPXoUzs7OGgmKiIioptGTSDSy6Sq1E5aRI0fik08+wcmTJyGRSJCSkoKNGzdi0qRJGD16dFXESEREpPP0NLTpKrWHhD799FOUlJSgW7duePz4MTp16gSpVIpJkyZh7NixVREjERER1XBqJywSiQSff/45Jk+ejOvXryM7Oxuenp4wN6/cw26IiIhqMs5hUe2lH81vZGSk+FAjIiIiqhw9VH4Oih50N2NRO2Hp2rWrygfTREdHVyogIiIiomepnbC0bNlS6XVhYSFiY2Nx8eJFhISEaCouIiKiGoVDQqqpnbAsXry43P3h4eHIzs6udEBEREQ1kSaeVMsn3VbAe++9h7Vr12qqOiIiIiKFl550+6yYmBgYGxtrqjoiIqIaRSJBpSfdckjoKf369VN6LQgCUlNTcebMGcyYMUNjgREREdUknMOimtoJi6WlpdJrPT09uLu7Y86cOfD399dYYERERESl1EpYiouLMXz4cHh5eaFWrVpVFRMREVGNw0m3qqk16VZfXx/+/v78VGYiIiINk2jon65Se5VQs2bNcOPGjaqIhYiIqMYq7WGp7Kar1E5YvvjiC0yaNAmRkZFITU2FXC5X2oiIiIg0rcJzWObMmYOJEyfirbfeAgD07t1b6RH9giBAIpGguLhY81ESERHpOM5hUa3CCcvs2bMxatQoHDx4sCrjISIiqpEkEonKz+qraB26qsIJiyAIAIDOnTtXWTBERERE5VFrWbMuZ25ERETaxCEh1dRKWNzc3F6YtGRkZFQqICIiopqIT7pVTa2EZfbs2WWedEtERERU1dRKWIKDg1GvXr2qioWIiKjG0pNIKv3hh5U9X8wqnLBw/goREVHV4RwW1Sr84LjSVUJEREREr1qFe1hKSkqqMg4iIqKaTQOTbnX4o4TUm8NCREREVUMPEuhVMuOo7PlixoSFiIhIBLisWTW1P/yQiIiI6FVjDwsREZEIcJWQakxYqFyrtx7Gsp8P4O4DOZq51sf/Jr8D76aNtB2WzmJ7a87j3Hys2bwfR09exkN5Dlwb2WHs+4Fo4tIAADD/u9+x99A5pXPatnTFwukhitf/3kjB9z/vxdXrd6CvJ0GnDk3xUUgPmJpIX+m96Ipj/1zHsp/24/zVJKTdl+PnhSMR2KWFtsMSHT6HRbVqMSQkkUiwY8cOAMDNmzchkUgQGxsLADh06BAkEgkyMzM1cq2IiAhYWVlppK7qatu+s5j+7XZMDe2BQz9NRTPX+ggauxz3Mh5pOzSdxPbWrIUrt+Ps+QR89nF/rP1mLNq0cMHEOetw74FcUaZdS1f8vnqqYps5boDi2P0MOSbOWYf6trWxcv6HWDA9BDeT7+Kr5du0cTs64XFuPpq51cfCKQO1HQo948iRI+jVqxfs7e2V3mtLDRs2TPEp0qVb9+7dlcpkZGRg8ODBkMlksLKywogRI5Cdna1U5sKFC+jYsSOMjY3h4OCABQsWqB2rKBKW8hqkvEYpz2uvvYbU1FSNfWTAwIED8e+//2qkrupqxaZoDO37Ggb39kETZzssmhYMU2Mj/PxnjLZD00lsb83Jzy/E4ROX8eGQALTwdEIDu9oYPrAb6tvWxh/7TirKGRoaoHYtC8VmYW6iOBZzNh4G+noYF9oTDevXRROXBpjwQW8cOXEJt1MfaOO2qr03fZti+uhe6NmVvSqqlE66reymjpycHLRo0QLLly9/bpnu3bsjNTVVsf3yyy9KxwcPHoxLly4hKioKkZGROHLkCD744APFcblcDn9/fzg6OuLs2bNYuHAhwsPD8cMPP6gVq2iGhLp3745169Yp7ZNKX9z9amRkBFtbW43FYWJiAhMTkxcX1FEFhUWIvZqM8cP8Ffv09PTQuZ07TsclajEy3cT21qzikhKUlJTAyFD5V5uRkQHirtxSvI69lIi+78+HhbkJWjVzxoh3/WBpYQoAKCwsgoGBPvT09J463xAAEHf1FhrY1X4Fd0I1kR40MCSk5rLmHj16oEePHirLSKXS577PXrlyBXv27MHp06fRpk0bAMCyZcvw1ltv4euvv4a9vT02btyIgoICrF27FkZGRmjatCliY2OxaNEipcTmRUTRwwL81yBPb7Vq1Xrhec8OCZUO6ezYsQOurq4wNjZGQEAAkpOTFeecP38eXbt2hYWFBWQyGby9vXHmzBml8ytSVhc9yMxGcXEJ6lpbKO2vay3D3ae61Ekz2N6aZWoiRVM3B2z47SDuZ8hRXFyCfUdicfnfZGRkPumibtfSFZ+NDcKiWcPxwXv+OH85EVPnrUdx8ZOHY7byckZGZjY2//E3CguL8Cg7Fz9s3AcAyHjIYTqqHuRyudKWn5//0nUdOnQI9erVg7u7O0aPHo0HD/7raYyJiYGVlZUiWQEAPz8/6Onp4eTJk4oynTp1gpGRkaJMQEAA4uPj8fDhwwrHIZoeFk16/Pgx5s2bhw0bNsDIyAgfffQRgoODcezYMQBPuq9atWqFlStXQl9fH7GxsTA0NCy3LnXKAkB+fr7SD4Zczjcdolfps4/7Y8GK7ej/wQLo6enBzdkOb/g2x783UgAA3V5vrijr7GiLxo62GBS2CLGXEuHdvDGcHGwwbUwQlq/fjR82RkFfT4J+b/mglpU5P1ONqpQmn8Pi4OCgtH/WrFkIDw9Xu77u3bujX79+cHJyQkJCAj777DP06NEDMTEx0NfXR1paWpkPRTYwMIC1tTXS0tIAAGlpaXByclIqY2NjozhWkc4JQEQJS2RkJMzNzZX2ffbZZ/jss8/UrquwsBDfffcd2rdvDwBYv349PDw8cOrUKbRr1w5JSUmYPHkymjRpAgBwdXV9bl3qlAWA+fPnY/bs2WrHLBa1rcyhr69XZsLnvQw56tWWaSkq3cX21rz6trWxZE4ocvMK8Dg3H7VrWWD2os2wtyn/l6K9jTUsZaa4k/YA3s0bAwD8OraAX8cWyMjMhrHUEBKJBL9GHoO9jfWrvBWqYfRQ+WGP0vOTk5Mhk/33O6QiUyzKExwcrPjay8sLzZs3R+PGjXHo0CF069atMqGqTTRDQl27dkVsbKzSNmrUqJeqy8DAAG3btlW8btKkCaysrHDlyhUAwIQJExAaGgo/Pz989dVXSEhIeG5d6pQFgGnTpiErK0uxPT0UVR0YGRqgZRMHHD4dr9hXUlKCI6f/RVsvJxVn0stge1cdE2Mj1K5lgUfZuTgVex2+bT3KLXf3QRbkj3JRu5ZFmWPWVuYwNZHi4LE4GBkawLtF46oOm0gjZDKZ0vayCcuznJ2dUadOHVy/fh0AYGtri7t37yqVKSoqQkZGhmLei62tLdLT05XKlL5WZw6qaBIWMzMzuLi4KG3W1lXz10x4eDguXbqEwMBAREdHw9PTE9u3b690WeBJFvvsD0p189GgN7Bhx3H8EnkC8YlpmPDVFuTk5mNwrw7aDk0nsb0161TsNZw89y9S0zNw5vx1jAtfg4b166BH19Z4nJuPlRv24NK/yUi9+xBnLyRg+v82or6tNdq2/K/3dNvuE/j3RgqSU+5j++4TWLImEiMH+cPCrOZOyK+M7Mf5iIu/jbj42wCAWykPEBd/G8lpGVqOTFzKWy37MltVun37Nh48eAA7OzsAgI+PDzIzM3H27FlFmejoaJSUlChGOXx8fHDkyBEUFhYqykRFRcHd3b3Cw0GAiIaENKmoqAhnzpxBu3btAADx8fHIzMyEh8d/f2G5ubnBzc0N48ePx7vvvot169bh7bffLrc+dcrqgn7+3rifmY0vv9+Fuw8ewcutPn5bGsYhiirC9tasnMd5WL1xH+49kMPC3ASdOjRF6LtvwsBAH8XFJbhxKw17D51D9uM81K5lgbYtXPB+sJ/SyqKr124jYssB5OYVoGH9upj4YW/4d26lxbuq3mKv3EKvUUsVrz9f/OSZNu8GtseK8CHaCkt0JKj8hy2re352draitwQAEhMTERsbC2tra1hbW2P27NkICgqCra0tEhISMGXKFLi4uCAgIAAA4OHhge7du2PkyJFYtWoVCgsLMWbMGAQHB8Pe3h4AMGjQIMyePRsjRozA1KlTcfHiRSxZsgSLFy9WK1bRJCz5+fmKCTqlDAwMUKdOHbXrMjQ0xNixY7F06VIYGBhgzJgx6NChA9q1a4fc3FxMnjwZ/fv3h5OTE27fvo3Tp08jKCioTD3qlNU1HwzojA8GdNZ2GDUG21tzur7mha6veZV7TCo1xMIZw15Yx2cf99dwVDXb695ueHj6O22HIXraeNLtmTNn0LVrV8XrCRMmAABCQkKwcuVKXLhwAevXr0dmZibs7e3h7++PuXPnKg0xbdy4EWPGjEG3bt2gp6eHoKAgLF36X4JqaWmJffv2ISwsDN7e3qhTpw5mzpyp1pJmQEQJy549exRdTKXc3d1x9epVtesyNTXF1KlTMWjQINy5cwcdO3bEmjVrAAD6+vp48OABhg4divT0dNSpUwf9+vUrd6KsOmWJiIiqmy5dukAQhOce37t37wvrsLa2xqZNm1SWad68Of7++2+143uaRFAVaTUUERGBcePGaexR/ZUll8thaWmJ9AdZ1XI+C1FFnL+Vqe0QapQWjlbaDqFGkcvlsKltiaysqvk9Xvo+8cOhyzA1Lzv5Wx2Psx/hgy6eVRarNommh4WIiKgm0+RzWHSRaFYJERERET2PziUsw4YNE81wEBERUUVVh2XN2sQhISIiIhHQ5JNudZEu3xsRERHpCPawEBERiYAmhnQ4JERERERVShtPuq1OOCREREREosceFiIiIhHgkJBqTFiIiIhEgKuEVGPCQkREJALsYVFNl5MxIiIi0hHsYSEiIhIBrhJSjQkLERGRCPDDD1XjkBARERGJHntYiIiIREAPEuhVclCnsueLGRMWIiIiEeCQkGocEiIiIiLRYw8LERGRCEj+/19l69BVTFiIiIhEgENCqnFIiIiIiESPPSxEREQiINHAKiEOCREREVGV4pCQakxYiIiIRIAJi2qcw0JERESixx4WIiIiEeCyZtWYsBAREYmAnuTJVtk6dBWHhIiIiEj02MNCREQkAhwSUo0JCxERkQhwlZBqHBIiIiIi0WMPCxERkQhIUPkhHR3uYGHCQkREJAZcJaQah4SIiIhI9JiwEBERiYBEQ//UceTIEfTq1Qv29vaQSCTYsWOH0nFBEDBz5kzY2dnBxMQEfn5+uHbtmlKZjIwMDB48GDKZDFZWVhgxYgSys7OVyly4cAEdO3aEsbExHBwcsGDBArXbhwkLERGRCJSuEqrspo6cnBy0aNECy5cvL/f4ggULsHTpUqxatQonT56EmZkZAgICkJeXpygzePBgXLp0CVFRUYiMjMSRI0fwwQcfKI7L5XL4+/vD0dERZ8+excKFCxEeHo4ffvhBrVg5h4WIiEgEJKj8pFl1z+/Rowd69OhR7jFBEPDtt99i+vTp6NOnDwBgw4YNsLGxwY4dOxAcHIwrV65gz549OH36NNq0aQMAWLZsGd566y18/fXXsLe3x8aNG1FQUIC1a9fCyMgITZs2RWxsLBYtWqSU2LwIe1iIiIiojMTERKSlpcHPz0+xz9LSEu3bt0dMTAwAICYmBlZWVopkBQD8/Pygp6eHkydPKsp06tQJRkZGijIBAQGIj4/Hw4cPKxwPe1iIiIhEQA8S6FXyyW96/9/HIpfLlfZLpVJIpVK16kpLSwMA2NjYKO23sbFRHEtLS0O9evWUjhsYGMDa2lqpjJOTU5k6So/VqlWrQvEwYSGiSmvhaKXtEGqUrMeF2g6hRnn0itpbk0NCDg4OSvtnzZqF8PDwStauXUxYiIiIdExycjJkMpnitbq9KwBga2sLAEhPT4ednZ1if3p6Olq2bKkoc/fuXaXzioqKkJGRoTjf1tYW6enpSmVKX5eWqQjOYSEiIhIDiYY2ADKZTGl7mYTFyckJtra2OHDggGKfXC7HyZMn4ePjAwDw8fFBZmYmzp49qygTHR2NkpIStG/fXlHmyJEjKCz8r6cqKioK7u7uFR4OApiwEBERiYI2nsOSnZ2N2NhYxMbGAngy0TY2NhZJSUmQSCQYN24cvvjiC/z555+Ii4vD0KFDYW9vj759+wIAPDw80L17d4wcORKnTp3CsWPHMGbMGAQHB8Pe3h4AMGjQIBgZGWHEiBG4dOkStmzZgiVLlmDChAlqxcohISIiohrqzJkz6Nq1q+J1aRIREhKCiIgITJkyBTk5Ofjggw+QmZmJ119/HXv27IGxsbHinI0bN2LMmDHo1q0b9PT0EBQUhKVLlyqOW1paYt++fQgLC4O3tzfq1KmDmTNnqrWkGQAkgiAIlbxfUkEul8PS0hLpD7KUxhOJiF4WJ92+Wo/kcrg61EFWVtX8Hi99nzgQmwRzi8rVn/1Ijm4tG1ZZrNrEHhYiIiIR0MaD46oTzmEhIiIi0WMPCxERkRiwi0UlJixEREQi8DKrfMqrQ1cxYSEiIhKBl/m05fLq0FWcw0JERESixx4WIiIiEeAUFtWYsBAREYkBMxaVOCREREREosceFiIiIhHgKiHVmLAQERGJAFcJqcYhISIiIhI99rAQERGJAOfcqsaEhYiISAyYsajEISEiIiISPfawEBERiQBXCanGhIWIiEgEuEpINSYsREREIsApLKpxDgsRERGJHntYiIiIxIBdLCoxYSEiIhIBTrpVjUNCREREJHrsYSEiIhIBrhJSjQkLERGRCHAKi2ocEiIiIiLRYw8LERGRGLCLRSUmLERERCLAVUKqcUiIiIiIRI89LERERCLAVUKqMWEhIiISAU5hUY0JCxERkRgwY1GJc1iIiIhI9NjDQkREJAJcJaQaExYiIiIx0MCkWx3OVzgkREREROJXLXpYJBIJtm/fjr59+2o7FJ137J/rWPbTfpy/moS0+3L8vHAkAru00HZYOm/11sNY9vMB3H0gRzPX+vjf5Hfg3bSRtsPSaWzzyvtpxzFs/OMYbqdlAABcG9ni45AAdO3gAQDIyy/EvBV/YGf0ORQUFqFT2yaYO74/6lpbKOo4fyUJ//shEnH/JkMCCVp4NMS0Ub3g6VJfK/ekTZxzq5rWe1jS0tIwduxYODs7QyqVwsHBAb169cKBAwe0HRoAIDw8HE2aNIGZmRlq1aoFPz8/nDx5UtthVZnHuflo5lYfC6cM1HYoNca2fWcx/dvtmBraA4d+mopmrvURNHY57mU80nZoOottrhl2dS0x9cOe2Ll6Iv78YQJea+2KDz5fg38TUwEAc7/bgQPHL2HF7GHYsmQM0u9nYdSMtYrzcx7nI2TK97CvVws7Vo7Hb9+NhbmpFEMnf4/ComJt3Zb2SDS0qSE8PBwSiURpa9KkieJ4Xl4ewsLCULt2bZibmyMoKAjp6elKdSQlJSEwMBCmpqaoV68eJk+ejKKiopdoANW0mrDcvHkT3t7eiI6OxsKFCxEXF4c9e/aga9euCAsL02ZoCm5ubvjuu+8QFxeHo0ePolGjRvD398e9e/e0HVqVeNO3KaaP7oWeXdmr8qqs2BSNoX1fw+DePmjibIdF04JhamyEn/+M0XZoOottrhl+vs3QtYMnnBrUhbNDPUweGQhTEynOXb4FeXYutv51EtPD+uC11q7wcnfAwk/fxdmLN/HPpZsAgISkdGTKH2PCiO5o3LAe3Jzs8ElIAO5nPMKd/++1oarXtGlTpKamKrajR48qjo0fPx47d+7Er7/+isOHDyMlJQX9+vVTHC8uLkZgYCAKCgpw/PhxrF+/HhEREZg5c6bG49RqwvLRRx9BIpHg1KlTCAoKgpubG5o2bYoJEybgxIkTzz1v6tSpcHNzg6mpKZydnTFjxgwUFhYqjp8/fx5du3aFhYUFZDIZvL29cebMGQDArVu30KtXL9SqVQtmZmZo2rQp/vrrr+dea9CgQfDz84OzszOaNm2KRYsWQS6X48KFC5prCKqxCgqLEHs1GV3auSv26enpoXM7d5yOS9RiZLqLbV41iotL8OeBf5Cbl4/WTRvh4r+3UVhUDF/v/9rZxdEG9W1qKRIW54b1UMvSDFt2nURBYRHy8guw5a+TcHG0QQNbay3difZINPRPXQYGBrC1tVVsderUAQBkZWVhzZo1WLRoEd544w14e3tj3bp1OH78uOI9et++fbh8+TJ+/vlntGzZEj169MDcuXOxfPlyFBQUaLR9tDaHJSMjA3v27MG8efNgZmZW5riVldVzz7WwsEBERATs7e0RFxeHkSNHwsLCAlOmTAEADB48GK1atcLKlSuhr6+P2NhYGBoaAgDCwsJQUFCAI0eOwMzMDJcvX4a5uXmFYi4oKMAPP/wAS0tLtGjBHgiqvAeZ2SguLlEa0weAutYyXLuZ/pyzqDLY5pp1NSEF/cKWIL+gCKYmRvj+i/fh2sgWl6/dgZGhPiwtTJTK16lloRh6Mzc1xuZvw/DB9LVYtmEfAKBRg7rYsPBDGBjov/J70TZNPppfLpcr7ZdKpZBKpeWec+3aNdjb28PY2Bg+Pj6YP38+GjZsiLNnz6KwsBB+fn6Ksk2aNEHDhg0RExODDh06ICYmBl5eXrCxsVGUCQgIwOjRo3Hp0iW0atWqcjf0FK0lLNevX4cgCEpjZRU1ffp0xdeNGjXCpEmTsHnzZkXCkpSUhMmTJyvqdnV1VZRPSkpCUFAQvLy8AADOzs4vvF5kZCSCg4Px+PFj2NnZISoqSpGBPis/Px/5+fmK18/+0BAR6RLnhvXw14+T8CgnD38dPo+JX27ClqVjKnRuXn4BpizYDO9mjbB0xhAUl5Rg9ZaDeP/T1fjz+/EwlhpVcfS6y8HBQen1rFmzEB4eXqZc+/btERERAXd3d6SmpmL27Nno2LEjLl68iLS0NBgZGZXpQLCxsUFaWhqAJ/NQn05WSo+XHtMkrSUsgiC89LlbtmzB0qVLkZCQgOzsbBQVFUEmkymOT5gwAaGhofjpp5/g5+eHd955B40bNwYAfPzxxxg9ejT27dsHPz8/BAUFoXnz5iqv17VrV8TGxuL+/ftYvXo1BgwYgJMnT6JevXplys6fPx+zZ89+6XujmqW2lTn09fXKTPa8lyFHvdqy55xFlcE21ywjQwM0alAXAODl7oALV5Ow9rcj6PVGKxQUFiPrUa5SL8v9h48UvVt/7P8Hd9IysH3FJ9DTezJDYcmMIWjR83PsO3oRvbu1fvU3pEWaXCWUnJys9L74vN6VHj16KL5u3rw52rdvD0dHR2zduhUmJiblnqMtWpvD4urqColEgqtXr6p1XkxMDAYPHoy33noLkZGROHfuHD7//HOlsbLw8HBcunQJgYGBiI6OhqenJ7Zv3w4ACA0NxY0bNzBkyBDExcWhTZs2WLZsmcprmpmZwcXFBR06dMCaNWtgYGCANWvWlFt22rRpyMrKUmzJyclq3R/VLEaGBmjZxAGHT8cr9pWUlODI6X/R1stJi5HpLrZ51SopEVBQWIRmbg1gaKCP4//8qziWkHQXd9IfovX/Lx/PzStQrEwppSeRQCIBhJKX/6O22tLgKiGZTKa0PS9heZaVlRXc3Nxw/fp12NraoqCgAJmZmUpl0tPTYWtrCwCwtbUts2qo9HVpGU3RWsJibW2NgIAALF++HDk5OWWOP9tApY4fPw5HR0d8/vnnaNOmDVxdXXHr1q0y5dzc3DB+/Hjs27cP/fr1w7p16xTHHBwcMGrUKGzbtg0TJ07E6tWr1Yq9pKREadjnaVKptMwPSnWS/TgfcfG3ERd/GwBwK+UB4uJvI5kz9qvMR4PewIYdx/FL5AnEJ6ZhwldbkJObj8G9Omg7NJ3FNteM//0QiZPnE5CcmoGrCSn43w+ROBGbgL5+3pCZm2DAW+3xxfI/cPyfa4iLT8bkr35B66aNFAnL623ckZWdixmLf8f1m+n4NzEVk7/6Bfr6evBp7aLdm9MCbU26fVp2djYSEhJgZ2cHb29vGBoaKj1mJD4+HklJSfDx8QEA+Pj4IC4uDnfv3lWUiYqKgkwmg6enZ6VieZZWHxy3fPly+Pr6ol27dpgzZw6aN2+OoqIiREVFYeXKlbhy5UqZc1xdXZGUlITNmzejbdu22LVrl6L3BAByc3MxefJk9O/fH05OTrh9+zZOnz6NoKAgAMC4cePQo0cPuLm54eHDhzh48CA8PDzKjS8nJwfz5s1D7969YWdnh/v372P58uW4c+cO3nnnnappFC2LvXILvUYtVbz+fPE2AMC7ge2xInyItsLSaf38vXE/Mxtffr8Ldx88gpdbffy2NIzDE1WIba4ZDx5mY8KXG3HvgRwWZiZo0tgOGxZ+iI5tn6wMmjGmL/T0JBg9M+L/Hxznjrnj+yvOd3G0wZovQ7Fk/V68HfYt9CR6aOpaH+sXfIh6tS21dVs1yqRJk9CrVy84OjoiJSUFs2bNgr6+Pt59911YWlpixIgRmDBhAqytrSGTyTB27Fj4+PigQ4cnyb2/vz88PT0xZMgQLFiwAGlpaZg+fTrCwsIq3KtTURKhMpNJNCA1NRXz5s1DZGQkUlNTUbduXXh7e2P8+PHo0qXLkyCfedLtlClTsHbtWuTn5yMwMBAdOnRAeHg4MjMzUVBQgJCQEBw7dgzp6emoU6cO+vXrh4ULF8LY2Bhjx47F7t27cfv2bchkMnTv3h2LFy9G7dq1y8SWl5eHQYMG4eTJk7h//z5q166Ntm3bYvr06Wjbtm2F7k8ul8PS0hLpD7KqXW8LEYlT1uPCFxcijXkkl8PVoQ6ysqrm93jp+8TFxLuwqGT9j+RyNHOqV+FYg4ODceTIETx48AB169bF66+/jnnz5inmfebl5WHixIn45ZdfkJ+fj4CAAKxYsUJpuOfWrVsYPXo0Dh06BDMzM4SEhOCrr76CgYFm+0S0nrDoOiYsRKRpTFherVeVsFzSUMLSVI2EpTrR+qP5iYiIiF6kWnz4IRERka7T5IPjdBETFiIiIlHg5zWrwiEhIiIiEj32sBAREYkAh4RUY8JCREQkAhwQUo1DQkRERCR67GEhIiISAQ4JqcaEhYiISAQ08VlAlT1fzJiwEBERiQEnsajEOSxEREQkeuxhISIiEgF2sKjGhIWIiEgEOOlWNQ4JERERkeixh4WIiEgEuEpINSYsREREYsBJLCpxSIiIiIhEjz0sREREIsAOFtWYsBAREYkAVwmpxiEhIiIiEj32sBAREYlC5VcJ6fKgEBMWIiIiEeCQkGocEiIiIiLRY8JCREREoschISIiIhHgkJBqTFiIiIhEgI/mV41DQkRERCR67GEhIiISAQ4JqcaEhYiISAT4aH7VOCREREREosceFiIiIjFgF4tKTFiIiIhEgKuEVOOQEBEREYkee1iIiIhEgKuEVGPCQkREJAKcwqIah4SIiIjEQKKh7SUsX74cjRo1grGxMdq3b49Tp05V6laqAhMWIiKiGmzLli2YMGECZs2ahX/++QctWrRAQEAA7t69q+3QlDBhISIiEgGJhv6pa9GiRRg5ciSGDx8OT09PrFq1Cqampli7dm0V3OXLY8JCREQkAqWTbiu7qaOgoABnz56Fn5+fYp+enh78/PwQExOj4TusHE66rWKCIAAAHsnlWo6EiHTFo8eF2g6hRnn06BGA/36fVxW5Bt4nSut4ti6pVAqpVFqm/P3791FcXAwbGxul/TY2Nrh69Wql49EkJixVrPQH3cXJQcuREBFRZTx69AiWlpYar9fIyAi2trZw1dD7hLm5ORwclOuaNWsWwsPDNVK/tjBhqWL29vZITk6GhYUFJNVogbxcLoeDgwOSk5Mhk8m0HU6NwDZ/tdjer1Z1bm9BEPDo0SPY29tXSf3GxsZITExEQUGBRuoTBKHM+015vSsAUKdOHejr6yM9PV1pf3p6OmxtbTUSj6YwYalienp6aNCggbbDeGkymaza/XKp7tjmrxbb+9Wqru1dFT0rTzM2NoaxsXGVXqM8RkZG8Pb2xoEDB9C3b18AQElJCQ4cOIAxY8a88nhUYcJCRERUg02YMAEhISFo06YN2rVrh2+//RY5OTkYPny4tkNTwoSFiIioBhs4cCDu3buHmTNnIi0tDS1btsSePXvKTMTVNiYsVC6pVIpZs2Y9d9yTNI9t/mqxvV8ttre4jRkzRnRDQM+SCFW9TouIiIiokvjgOCIiIhI9JixEREQkekxYiIiISPSYsNQAEokEO3bs0HYYNQrb/NV6ur1v3rwJiUSC2NhYAMChQ4cgkUiQmZmpkWtFRETAyspKI3VVZ/wZp1eNCUs1l5aWhrFjx8LZ2RlSqRQODg7o1asXDhw4oO3QADx54uLMmTNhZ2cHExMT+Pn54dq1a9oOq1LE3ubbtm2Dv78/ateurfTGXZ0NGzYMEomkzNa9e/cXnvvaa68hNTVVYw/+GjhwIP7991+N1CVWYv8ZDw8PR5MmTWBmZoZatWrBz88PJ0+e1HZYVMW4rLkau3nzJnx9fWFlZYWFCxfCy8sLhYWF2Lt3L8LCwkTxwVULFizA0qVLsX79ejg5OWHGjBkICAjA5cuXtfJUx8qqDm2ek5OD119/HQMGDMDIkSO1HY7GdO/eHevWrVPaV5ElsqWf06IpJiYmMDEx0Vh9YlMdfsbd3Nzw3XffwdnZGbm5uVi8eDH8/f1x/fp11K1bV9vhUVURqNrq0aOHUL9+fSE7O7vMsYcPHyq+BiBs375d8XrKlCmCq6urYGJiIjg5OQnTp08XCgoKFMdjY2OFLl26CObm5oKFhYXQunVr4fTp04IgCMLNmzeFnj17ClZWVoKpqang6ekp7Nq1q9z4SkpKBFtbW2HhwoWKfZmZmYJUKhV++eWXSt69doi9zZ+WmJgoABDOnTv30vcrFiEhIUKfPn2ee/zp9n72vg8ePCgAUHx/1q1bJ1haWgrbt28XXFxcBKlUKvj7+wtJSUmK+lR9P0rPr0jZ6qg6/YyXysrKEgAI+/fvV/+GqdpgD0s1lZGRgT179mDevHkwMzMrc1zVGLuFhQUiIiJgb2+PuLg4jBw5EhYWFpgyZQoAYPDgwWjVqhVWrlwJfX19xMbGwtDQEAAQFhaGgoICHDlyBGZmZrh8+TLMzc3LvU5iYiLS0tLg5+en2GdpaYn27dsjJiYGwcHBlWiBV686tDlVzOPHjzFv3jxs2LABRkZG+OijjxAcHIxjx44BUP39eJY6ZcWuOv6MFxQU4IcffoClpSVatGih/k1T9aHtjIlezsmTJwUAwrZt215YFs/8JfSshQsXCt7e3orXFhYWQkRERLllvby8hPDw8ArFeOzYMQGAkJKSorT/nXfeEQYMGFChOsSkOrT503Sth0VfX18wMzNT2ubNmycIgvo9LACEEydOKOq/cuWKAEA4efKkIAiqvx/P9rCoKlvdVKef8Z07dwpmZmaCRCIR7O3thVOnTql1PlU/nHRbTQmVeEDxli1b4OvrC1tbW5ibm2P69OlISkpSHJ8wYQJCQ0Ph5+eHr776CgkJCYpjH3/8Mb744gv4+vpi1qxZuHDhQqXuozphm2tX165dERsbq7SNGjXqpeoyMDBA27ZtFa+bNGkCKysrXLlyBYDq78ez1CkrdtXpZ7z05+H48ePo3r07BgwYgLt37750/CR+TFiqKVdXV0gkErUnwMXExGDw4MF46623EBkZiXPnzuHzzz9HQUGBokx4eDguXbqEwMBAREdHw9PTE9u3bwcAhIaG4saNGxgyZAji4uLQpk0bLFu2rNxrlU50TE9PV9qfnp6u0UmQr0p1aHNdZmZmBhcXF6XN2tq6Sq6l6vtRmbJiV51+xkt/Hjp06IA1a9bAwMAAa9asUf+mqfrQcg8PVUL37t3Vnhz39ddfC87OzkplR4wYodTF/azg4GChV69e5R779NNPBS8vr3KPlU66/frrrxX7srKyqvWkW7G3+dN0bUhIk5Nu8dTwjyAIwtWrV8vse9rT349nh4RUla2OqtPP+NOcnZ2FWbNmqXUOVS/sYanGli9fjuLiYrRr1w6///47rl27hitXrmDp0qXw8fEp9xxXV1ckJSVh8+bNSEhIwNKlS5X+GszNzcWYMWNw6NAh3Lp1C8eOHcPp06fh4eEBABg3bhz27t2LxMRE/PPPPzh48KDi2LMkEgnGjRuHL774An/++Sfi4uIwdOhQ2Nvbo2/fvhpvj1dB7G0OPJk4GRsbi8uXLwMA4uPjERsbi7S0NA22xKuXn5+PtLQ0pe3+/fsvVZehoSHGjh2LkydP4uzZsxg2bBg6dOiAdu3avfD78TR1ylYXYv8Zz8nJwWeffYYTJ07g1q1bOHv2LN5//33cuXMH77zzjuYbhMRD2xkTVU5KSooQFhYmODo6CkZGRkL9+vWF3r17CwcPHlSUwTOT4yZPnizUrl1bMDc3FwYOHCgsXrxY8ZdQfn6+EBwcLDg4OAhGRkaCvb29MGbMGCE3N1cQBEEYM2aM0LhxY0EqlQp169YVhgwZIty/f/+58ZWUlAgzZswQbGxsBKlUKnTr1k2Ij4+viqZ4ZcTe5qU9CM9u1fmvz5CQkHLvyd3dXRCEl1vW/PvvvwvOzs6CVCoV/Pz8hFu3bgmC8OLvx9M9LC8qW12J+Wc8NzdXePvttwV7e3vByMhIsLOzE3r37s1JtzWARBAqMcuKiKiaiYiIwLhx4zT2qH4iejU4JERERESix4SFiIiIRI9DQkRERCR67GEhIiIi0WPCQkRERKLHhIWIiIhEjwkLERERiR4TFqIaYNiwYUpPF+7SpQvGjRv3yuM4dOgQJBKJymegSCQS7Nixo8J1hoeHo2XLlpWK6+bNm5BIJIiNja1UPURUdZiwEGnJsGHDIJFIIJFIYGRkBBcXF8yZMwdFRUVVfu1t27Zh7ty5FSpbkSSDiKiqGWg7AKKarHv37li3bh3y8/Px119/ISwsDIaGhpg2bVqZsgUFBTAyMtLIdavqU46JiKoKe1iItEgqlcLW1haOjo4YPXo0/Pz88OeffwL4bxhn3rx5sLe3h7u7OwAgOTkZAwYMgJWVFaytrdGnTx/cvHlTUWdxcTEmTJgAKysr1K5dG1OmTMGzj1t6dkgoPz8fU6dOhYODA6RSKVxcXLBmzRrcvHkTXbt2BQDUqlULEokEw4YNAwCUlJRg/vz5cHJygomJCVq0aIHffvtN6Tp//fUX3NzcYGJigq5duyrFWVFTp06Fm5sbTE1N4ezsjBkzZqCwsLBMue+//x4ODg4wNTXFgAEDkJWVpXT8xx9/hIeHB4yNjdGkSROsWLFC7ViISHuYsBCJiImJCQoKChSvDxw4gPj4eERFRSEyMhKFhYUICAiAhYUF/v77bxw7dgzm5ubo3r274rxvvvkGERERWLt2LY4ePYqMjAylT84tz9ChQ/HLL79g6dKluHLlCr7//nuYm5vDwcEBv//+O4Ann/qcmpqKJUuWAADmz5+PDRs2YNWqVbh06RLGjx+P9957D4cPHwbwJLHq168fevXqhdjYWISGhuLTTz9Vu00sLCwQERGBy5cvY8mSJVi9ejUWL16sVOb69evYunUrdu7ciT179uDcuXP46KOPFMc3btyImTNnYt68ebhy5Qq+/PJLzJgxA+vXr1c7HiLSEq1+9CJRDRYSEiL06dNHEIQnn2odFRUlSKVSYdKkSYrjNjY2Qn5+vuKcn376SXB3dxdKSkoU+/Lz8wUTExNh7969giAIgp2dnbBgwQLF8cLCQqFBgwaKawmCIHTu3Fn45JNPBEEQhPj4eAGAEBUVVW6cz37asSAIQl5enmBqaiocP35cqeyIESOEd999VxAEQZg2bZrg6empdHzq1Kll6noWnvkU4GctXLhQ8Pb2VryeNWuWoK+vL9y+fVuxb/fu3YKenp6QmpoqCIIgNG7cWNi0aZNSPXPnzhV8fHwEQSj7Cc9EJD6cw0KkRZGRkTA3N0dhYSFKSkowaNAghIeHK457eXkpzVs5f/48rl+/DgsLC6V68vLykJCQgKysLKSmpqJ9+/aKYwYGBmjTpk2ZYaFSsbGx0NfXR+fOnSsc9/Xr1/H48WO8+eabSvsLCgrQqlUrAMCVK1eU4gAAHx+fCl+j1JYtW7B06VIkJCQgOzsbRUVFkMlkSmUaNmyI+vXrK12npKQE8fHxsLCwQEJCAkaMGIGRI0cqyhQVFcHS0lLteIhIO5iwEGlR165dsXLlShgZGcHe3h4GBsr/Jc3MzJReZ2dnw9vbGxs3bixTV926dV8qBhMTE7XPyc7OBgDs2rVLKVEAnszL0ZSYmBgMHjwYs2fPRkBAACwtLbF582Z88803ase6evXqMgmUvr6+xmIloqrFhIVIi8zMzODi4lLh8q1bt8aWLVtQr169Mr0Mpezs7HDy5El06tQJwJOehLNnz6J169bllvfy8kJJSQkOHz4MPz+/MsdLe3iKi4sV+zw9PSGVSpGUlPTcnhkPDw/FBOJSJ06cePFNPuX48eNwdHTE559/rth369atMuWSkpKQkpICe3t7xXX09PTg7u4OGxsb2Nvb48aNGxg8eLBa1yci8eCkW6JqZPDgwahTpw769OmDv//+G4mJiTh06BA+/vhj3L59GwDwySef4KuvvsKOHTtw9epVfPTRRyqfodKoUSOEhITg/fffx44dOxR1bt26FQDg6OgIiUSCyMhI3Lt3D9nZ2bCwsMCkSZMwfvx4rF+/HgkJCfjnn3+wbNkyxUTWUaNG4dq1a5g8eTLi4+OxadMmREREqHW/rq6uSEpKwubNm5GQkIClS5eWO4HY2NgYISEhOH/+PP7++298/PHHGDBgAGxtbQEAs2fPxvz587F06VL8+++/iIuLw7p167Bo0SK14iEi7WHCQlSNmJqa4siRI2jYsCH69esHDw8PjBgxAnl5eYoel4kTJ2LIkCEICQmBj48PLCws8Pbbb6usd+XKlejfvz8++ugjNGnSBCNHjkROTg4AoH79+pg9ezY+/fRT2NjYYMyYMQCAuXPnYsaMGZg/fz48PDzQvXt37Nq1C05OTgCezCv5/fffsWPHDrRo0QKrVq3Cl19+qdb99u7dG+PHj8eYMWPQsmVLHD9+HDNmzChTzsXFBf369cNbb70Ff39/NG/eXGnZcmhoKH788UesW7cOXl5e6Ny5MyIiIhSxEpH4SYTnzcQjIiIiEgn2sBAREZHoMWEhIiIi0WPCQkRERKLHhIWIiIhEjwkLERERiR4TFiIiIhI9JixEREQkekxYiIiISPSYsBAREZHoMWEhIiIi0WPCQkRERKLHhIWIiIhE7/8Aw2Zi2cxRyKAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.84%\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class 0 - Precision: 0.9992, Recall: 0.9982, F1-Score: 0.9987\n",
      "Class 1 - Precision: 0.9956, Recall: 0.9987, F1-Score: 0.9971\n",
      "Class 2 - Precision: 1.0000, Recall: 0.9990, F1-Score: 0.9995\n",
      "Class 3 - Precision: 0.9968, Recall: 0.9968, F1-Score: 0.9968\n",
      "Multi-Class ROC-AUC (OVR): 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model (same architecture as trained)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),  # Lower dropout\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),         # Consistent activation\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),\n",
    "    nn.Linear(16, output_size)  # No Softmax here\n",
    ")\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load(f'{model_save_dir}/final_model.pt'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "model.to(device)\n",
    "\n",
    "# Test data preparation\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize variables to store results\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_probs = []  # To store probabilities for metrics like ROC-AUC\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Predicted labels\n",
    "        probabilities = torch.softmax(outputs, dim=1)  # Probabilities for each class\n",
    "\n",
    "        # Append results to lists\n",
    "        y_true.extend(labels.cpu().numpy())  # Ground truth\n",
    "        y_pred.extend(predicted.cpu().numpy())  # Predicted labels\n",
    "        y_probs.extend(probabilities.cpu().numpy())  # Class probabilities\n",
    "\n",
    "# Convert results to NumPy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_probs = np.array(y_probs)  # Probabilities for each class\n",
    "\n",
    "# **1. Classification Report**\n",
    "print(\"\\nClassification Report:\")\n",
    "\n",
    "# **2. Confusion Matrix**\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\", ..., f\"Class {output_size-1}\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# **3. Accuracy**\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# **4. Precision, Recall, and F1-Score**\n",
    "precision = cm.diagonal() / cm.sum(axis=0)\n",
    "recall = cm.diagonal() / cm.sum(axis=1)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for i, (p, r, f1) in enumerate(zip(precision, recall, f1_scores)):\n",
    "    print(f\"Class {i} - Precision: {p:.4f}, Recall: {r:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# **5. ROC-AUC (for multi-class)**\n",
    "if output_size == 2:\n",
    "    # For binary classification\n",
    "    roc_auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs[:, 1])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    # For multi-class ROC-AUC (one-vs-rest approach)\n",
    "    roc_auc_ovr = roc_auc_score(y_true, y_probs, multi_class='ovr')\n",
    "    print(f\"Multi-Class ROC-AUC (OVR): {roc_auc_ovr:.4f}\")\n",
    "\n",
    "# **6. Log Predictions (optional for debugging)**\n",
    "# Save the predictions for analysis\n",
    "np.savetxt(\"predictions.csv\", np.column_stack((y_true, y_pred)), fmt='%d', delimiter=',', header='True,Predicted')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
