{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>age_years</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp_category</th>\n",
       "      <th>bp_category_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>21.967120</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>34.927679</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>23.507805</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>28.710479</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>23.011177</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68200</th>\n",
       "      <td>99993</td>\n",
       "      <td>19240</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>26.927438</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68201</th>\n",
       "      <td>99995</td>\n",
       "      <td>22601</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>50.472681</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68202</th>\n",
       "      <td>99996</td>\n",
       "      <td>19066</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>31.353579</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "      <td>Hypertension Stage 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68203</th>\n",
       "      <td>99998</td>\n",
       "      <td>22431</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>27.099251</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68204</th>\n",
       "      <td>99999</td>\n",
       "      <td>20540</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>24.913495</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "      <td>Hypertension Stage 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68205 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  \\\n",
       "0          0  18393       2     168    62.0    110     80            1     1   \n",
       "1          1  20228       1     156    85.0    140     90            3     1   \n",
       "2          2  18857       1     165    64.0    130     70            3     1   \n",
       "3          3  17623       2     169    82.0    150    100            1     1   \n",
       "4          4  17474       1     156    56.0    100     60            1     1   \n",
       "...      ...    ...     ...     ...     ...    ...    ...          ...   ...   \n",
       "68200  99993  19240       2     168    76.0    120     80            1     1   \n",
       "68201  99995  22601       1     158   126.0    140     90            2     2   \n",
       "68202  99996  19066       2     183   105.0    180     90            3     1   \n",
       "68203  99998  22431       1     163    72.0    135     80            1     2   \n",
       "68204  99999  20540       1     170    72.0    120     80            2     1   \n",
       "\n",
       "       smoke  alco  active  cardio  age_years        bmi  \\\n",
       "0          0     0       1       0         50  21.967120   \n",
       "1          0     0       1       1         55  34.927679   \n",
       "2          0     0       0       1         51  23.507805   \n",
       "3          0     0       1       1         48  28.710479   \n",
       "4          0     0       0       0         47  23.011177   \n",
       "...      ...   ...     ...     ...        ...        ...   \n",
       "68200      1     0       1       0         52  26.927438   \n",
       "68201      0     0       1       1         61  50.472681   \n",
       "68202      0     1       0       1         52  31.353579   \n",
       "68203      0     0       0       1         61  27.099251   \n",
       "68204      0     0       1       0         56  24.913495   \n",
       "\n",
       "                bp_category   bp_category_encoded  \n",
       "0      Hypertension Stage 1  Hypertension Stage 1  \n",
       "1      Hypertension Stage 2  Hypertension Stage 2  \n",
       "2      Hypertension Stage 1  Hypertension Stage 1  \n",
       "3      Hypertension Stage 2  Hypertension Stage 2  \n",
       "4                    Normal                Normal  \n",
       "...                     ...                   ...  \n",
       "68200  Hypertension Stage 1  Hypertension Stage 1  \n",
       "68201  Hypertension Stage 2  Hypertension Stage 2  \n",
       "68202  Hypertension Stage 2  Hypertension Stage 2  \n",
       "68203  Hypertension Stage 1  Hypertension Stage 1  \n",
       "68204  Hypertension Stage 1  Hypertension Stage 1  \n",
       "\n",
       "[68205 rows x 17 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('cardio_data_processed.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to normalize\n",
    "features_to_normalize = ['age', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "# Extract the feature columns from the dataframe\n",
    "X = df[features_to_normalize]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            age    height    weight     ap_hi     ap_lo  cholesterol  gluc  \\\n",
      "0      0.588076  0.579487  0.269841  0.222222  0.333333          0.0   0.0   \n",
      "1      0.730159  0.517949  0.391534  0.555556  0.500000          1.0   0.0   \n",
      "2      0.624003  0.564103  0.280423  0.444444  0.166667          1.0   0.0   \n",
      "3      0.528455  0.584615  0.375661  0.666667  0.666667          0.0   0.0   \n",
      "4      0.516918  0.517949  0.238095  0.111111  0.000000          0.0   0.0   \n",
      "...         ...       ...       ...       ...       ...          ...   ...   \n",
      "68200  0.653659  0.579487  0.343915  0.333333  0.333333          0.0   0.0   \n",
      "68201  0.913899  0.528205  0.608466  0.555556  0.500000          0.5   0.5   \n",
      "68202  0.640186  0.656410  0.497354  1.000000  0.500000          1.0   0.0   \n",
      "68203  0.900736  0.553846  0.322751  0.500000  0.333333          0.0   0.5   \n",
      "68204  0.754317  0.589744  0.322751  0.333333  0.333333          0.5   0.0   \n",
      "\n",
      "       smoke  alco  active  \n",
      "0        0.0   0.0     1.0  \n",
      "1        0.0   0.0     1.0  \n",
      "2        0.0   0.0     0.0  \n",
      "3        0.0   0.0     1.0  \n",
      "4        0.0   0.0     0.0  \n",
      "...      ...   ...     ...  \n",
      "68200    1.0   0.0     1.0  \n",
      "68201    0.0   0.0     1.0  \n",
      "68202    0.0   1.0     0.0  \n",
      "68203    0.0   0.0     0.0  \n",
      "68204    0.0   0.0     1.0  \n",
      "\n",
      "[68205 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame for better readability\n",
    "X_normalized_df = pd.DataFrame(X_normalized, columns=features_to_normalize)\n",
    "\n",
    "# Show the normalized data\n",
    "print(X_normalized_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.588076</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.391534</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.624003</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.280423</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.375661</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.516918</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68200</th>\n",
       "      <td>0.653659</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.343915</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68201</th>\n",
       "      <td>0.913899</td>\n",
       "      <td>0.528205</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68202</th>\n",
       "      <td>0.640186</td>\n",
       "      <td>0.656410</td>\n",
       "      <td>0.497354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68203</th>\n",
       "      <td>0.900736</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68204</th>\n",
       "      <td>0.754317</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68205 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    height    weight     ap_hi     ap_lo  cholesterol  gluc  \\\n",
       "0      0.588076  0.579487  0.269841  0.222222  0.333333          0.0   0.0   \n",
       "1      0.730159  0.517949  0.391534  0.555556  0.500000          1.0   0.0   \n",
       "2      0.624003  0.564103  0.280423  0.444444  0.166667          1.0   0.0   \n",
       "3      0.528455  0.584615  0.375661  0.666667  0.666667          0.0   0.0   \n",
       "4      0.516918  0.517949  0.238095  0.111111  0.000000          0.0   0.0   \n",
       "...         ...       ...       ...       ...       ...          ...   ...   \n",
       "68200  0.653659  0.579487  0.343915  0.333333  0.333333          0.0   0.0   \n",
       "68201  0.913899  0.528205  0.608466  0.555556  0.500000          0.5   0.5   \n",
       "68202  0.640186  0.656410  0.497354  1.000000  0.500000          1.0   0.0   \n",
       "68203  0.900736  0.553846  0.322751  0.500000  0.333333          0.0   0.5   \n",
       "68204  0.754317  0.589744  0.322751  0.333333  0.333333          0.5   0.0   \n",
       "\n",
       "       smoke  alco  active  \n",
       "0        0.0   0.0     1.0  \n",
       "1        0.0   0.0     1.0  \n",
       "2        0.0   0.0     0.0  \n",
       "3        0.0   0.0     1.0  \n",
       "4        0.0   0.0     0.0  \n",
       "...      ...   ...     ...  \n",
       "68200    1.0   0.0     1.0  \n",
       "68201    0.0   0.0     1.0  \n",
       "68202    0.0   1.0     0.0  \n",
       "68203    0.0   0.0     0.0  \n",
       "68204    0.0   0.0     1.0  \n",
       "\n",
       "[68205 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hypertension Stage 1', 'Hypertension Stage 2', 'Normal',\n",
       "       'Elevated'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bp_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min': [10798.0, 55.0, 11.0, 90.0, 60.0, 1.0, 1.0, 0.0, 0.0, 0.0], 'scale': [12915.0, 195.0, 189.0, 90.0, 60.0, 2.0, 2.0, 1.0, 1.0, 1.0]}\n"
     ]
    }
   ],
   "source": [
    "# Extract the min and scale values from the scaler\n",
    "scaler_params = {\n",
    "    'min': scaler.data_min_.tolist(),   # The minimum values for each feature\n",
    "    'scale': scaler.data_range_.tolist() # The scaling factors for each feature\n",
    "}\n",
    "\n",
    "# Save the parameters as a JSON file\n",
    "import json\n",
    "\n",
    "with open('minmax_scaler_params.json', 'w') as json_file:\n",
    "    json.dump(scaler_params, json_file)\n",
    "\n",
    "print(scaler_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id    age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  \\\n",
      "0          0  18393       2     168    62.0    110     80            1     1   \n",
      "1          1  20228       1     156    85.0    140     90            3     1   \n",
      "2          2  18857       1     165    64.0    130     70            3     1   \n",
      "3          3  17623       2     169    82.0    150    100            1     1   \n",
      "4          4  17474       1     156    56.0    100     60            1     1   \n",
      "...      ...    ...     ...     ...     ...    ...    ...          ...   ...   \n",
      "68200  99993  19240       2     168    76.0    120     80            1     1   \n",
      "68201  99995  22601       1     158   126.0    140     90            2     2   \n",
      "68202  99996  19066       2     183   105.0    180     90            3     1   \n",
      "68203  99998  22431       1     163    72.0    135     80            1     2   \n",
      "68204  99999  20540       1     170    72.0    120     80            2     1   \n",
      "\n",
      "       smoke  alco  active  cardio  age_years        bmi  \\\n",
      "0          0     0       1       0         50  21.967120   \n",
      "1          0     0       1       1         55  34.927679   \n",
      "2          0     0       0       1         51  23.507805   \n",
      "3          0     0       1       1         48  28.710479   \n",
      "4          0     0       0       0         47  23.011177   \n",
      "...      ...   ...     ...     ...        ...        ...   \n",
      "68200      1     0       1       0         52  26.927438   \n",
      "68201      0     0       1       1         61  50.472681   \n",
      "68202      0     1       0       1         52  31.353579   \n",
      "68203      0     0       0       1         61  27.099251   \n",
      "68204      0     0       1       0         56  24.913495   \n",
      "\n",
      "                bp_category  bp_category_encoded  \n",
      "0      Hypertension Stage 1                    0  \n",
      "1      Hypertension Stage 2                    1  \n",
      "2      Hypertension Stage 1                    0  \n",
      "3      Hypertension Stage 2                    1  \n",
      "4                    Normal                    2  \n",
      "...                     ...                  ...  \n",
      "68200  Hypertension Stage 1                    0  \n",
      "68201  Hypertension Stage 2                    1  \n",
      "68202  Hypertension Stage 2                    1  \n",
      "68203  Hypertension Stage 1                    0  \n",
      "68204  Hypertension Stage 1                    0  \n",
      "\n",
      "[68205 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a mapping dictionary for bp_category\n",
    "bp_category_mapping = {label: idx for idx, label in enumerate(df['bp_category'].unique())}\n",
    "\n",
    "# Step 2: Encode the 'bp_category' column using the mapping\n",
    "df['bp_category_encoded'] = df['bp_category'].map(bp_category_mapping)\n",
    "\n",
    "# Show the encoded dataframe\n",
    "print(df)\n",
    "\n",
    "# Step 3: Save the mapping as a JSON file\n",
    "with open('bp_category_mapping.json', 'w') as json_file:\n",
    "    json.dump(bp_category_mapping, json_file)\n",
    "\n",
    "# Optionally, save the dataframe with the encoded column if needed\n",
    "df.to_csv('encoded_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.588076</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.391534</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.624003</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.280423</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.375661</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.516918</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68200</th>\n",
       "      <td>0.653659</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.343915</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68201</th>\n",
       "      <td>0.913899</td>\n",
       "      <td>0.528205</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68202</th>\n",
       "      <td>0.640186</td>\n",
       "      <td>0.656410</td>\n",
       "      <td>0.497354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68203</th>\n",
       "      <td>0.900736</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68204</th>\n",
       "      <td>0.754317</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68205 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    height    weight     ap_hi     ap_lo  cholesterol  gluc  \\\n",
       "0      0.588076  0.579487  0.269841  0.222222  0.333333          0.0   0.0   \n",
       "1      0.730159  0.517949  0.391534  0.555556  0.500000          1.0   0.0   \n",
       "2      0.624003  0.564103  0.280423  0.444444  0.166667          1.0   0.0   \n",
       "3      0.528455  0.584615  0.375661  0.666667  0.666667          0.0   0.0   \n",
       "4      0.516918  0.517949  0.238095  0.111111  0.000000          0.0   0.0   \n",
       "...         ...       ...       ...       ...       ...          ...   ...   \n",
       "68200  0.653659  0.579487  0.343915  0.333333  0.333333          0.0   0.0   \n",
       "68201  0.913899  0.528205  0.608466  0.555556  0.500000          0.5   0.5   \n",
       "68202  0.640186  0.656410  0.497354  1.000000  0.500000          1.0   0.0   \n",
       "68203  0.900736  0.553846  0.322751  0.500000  0.333333          0.0   0.5   \n",
       "68204  0.754317  0.589744  0.322751  0.333333  0.333333          0.5   0.0   \n",
       "\n",
       "       smoke  alco  active  \n",
       "0        0.0   0.0     1.0  \n",
       "1        0.0   0.0     1.0  \n",
       "2        0.0   0.0     0.0  \n",
       "3        0.0   0.0     1.0  \n",
       "4        0.0   0.0     0.0  \n",
       "...      ...   ...     ...  \n",
       "68200    1.0   0.0     1.0  \n",
       "68201    0.0   0.0     1.0  \n",
       "68202    0.0   1.0     0.0  \n",
       "68203    0.0   0.0     0.0  \n",
       "68204    0.0   0.0     1.0  \n",
       "\n",
       "[68205 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  gender  cardio  age_years        bmi  bp_category_encoded  \\\n",
      "0          0       2       0         50  21.967120                    0   \n",
      "1          1       1       1         55  34.927679                    1   \n",
      "2          2       1       1         51  23.507805                    0   \n",
      "3          3       2       1         48  28.710479                    1   \n",
      "4          4       1       0         47  23.011177                    2   \n",
      "...      ...     ...     ...        ...        ...                  ...   \n",
      "68200  99993       2       0         52  26.927438                    0   \n",
      "68201  99995       1       1         61  50.472681                    1   \n",
      "68202  99996       2       1         52  31.353579                    1   \n",
      "68203  99998       1       1         61  27.099251                    0   \n",
      "68204  99999       1       0         56  24.913495                    0   \n",
      "\n",
      "            age    height    weight     ap_hi     ap_lo  cholesterol  gluc  \\\n",
      "0      0.588076  0.579487  0.269841  0.222222  0.333333          0.0   0.0   \n",
      "1      0.730159  0.517949  0.391534  0.555556  0.500000          1.0   0.0   \n",
      "2      0.624003  0.564103  0.280423  0.444444  0.166667          1.0   0.0   \n",
      "3      0.528455  0.584615  0.375661  0.666667  0.666667          0.0   0.0   \n",
      "4      0.516918  0.517949  0.238095  0.111111  0.000000          0.0   0.0   \n",
      "...         ...       ...       ...       ...       ...          ...   ...   \n",
      "68200  0.653659  0.579487  0.343915  0.333333  0.333333          0.0   0.0   \n",
      "68201  0.913899  0.528205  0.608466  0.555556  0.500000          0.5   0.5   \n",
      "68202  0.640186  0.656410  0.497354  1.000000  0.500000          1.0   0.0   \n",
      "68203  0.900736  0.553846  0.322751  0.500000  0.333333          0.0   0.5   \n",
      "68204  0.754317  0.589744  0.322751  0.333333  0.333333          0.5   0.0   \n",
      "\n",
      "       smoke  alco  active  \n",
      "0        0.0   0.0     1.0  \n",
      "1        0.0   0.0     1.0  \n",
      "2        0.0   0.0     0.0  \n",
      "3        0.0   0.0     1.0  \n",
      "4        0.0   0.0     0.0  \n",
      "...      ...   ...     ...  \n",
      "68200    1.0   0.0     1.0  \n",
      "68201    0.0   0.0     1.0  \n",
      "68202    0.0   1.0     0.0  \n",
      "68203    0.0   0.0     0.0  \n",
      "68204    0.0   0.0     1.0  \n",
      "\n",
      "[68205 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, combine the normalized data with the original dataframe\n",
    "# Drop the original columns that are being normalized\n",
    "df_combined = df.drop(columns=features_to_normalize)\n",
    "\n",
    "# Concatenate the normalized dataframe with the original dataframe (excluding normalized columns)\n",
    "df_combined = pd.concat([df_combined, X_normalized_df], axis=1)\n",
    "df_combined.drop(columns='bp_category', inplace=True)\n",
    "\n",
    "# Show the combined dataframe\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.drop(columns='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>cardio</th>\n",
       "      <th>bp_category_encoded</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.588076</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.391534</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.624003</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.280423</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.375661</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.516918</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68200</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.653659</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.343915</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68201</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.913899</td>\n",
       "      <td>0.528205</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68202</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.640186</td>\n",
       "      <td>0.656410</td>\n",
       "      <td>0.497354</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68203</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900736</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68204</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.754317</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68205 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gender  cardio  bp_category_encoded       age    height    weight  \\\n",
       "0           2       0                    0  0.588076  0.579487  0.269841   \n",
       "1           1       1                    1  0.730159  0.517949  0.391534   \n",
       "2           1       1                    0  0.624003  0.564103  0.280423   \n",
       "3           2       1                    1  0.528455  0.584615  0.375661   \n",
       "4           1       0                    2  0.516918  0.517949  0.238095   \n",
       "...       ...     ...                  ...       ...       ...       ...   \n",
       "68200       2       0                    0  0.653659  0.579487  0.343915   \n",
       "68201       1       1                    1  0.913899  0.528205  0.608466   \n",
       "68202       2       1                    1  0.640186  0.656410  0.497354   \n",
       "68203       1       1                    0  0.900736  0.553846  0.322751   \n",
       "68204       1       0                    0  0.754317  0.589744  0.322751   \n",
       "\n",
       "          ap_hi     ap_lo  cholesterol  gluc  smoke  alco  active  \n",
       "0      0.222222  0.333333          0.0   0.0    0.0   0.0     1.0  \n",
       "1      0.555556  0.500000          1.0   0.0    0.0   0.0     1.0  \n",
       "2      0.444444  0.166667          1.0   0.0    0.0   0.0     0.0  \n",
       "3      0.666667  0.666667          0.0   0.0    0.0   0.0     1.0  \n",
       "4      0.111111  0.000000          0.0   0.0    0.0   0.0     0.0  \n",
       "...         ...       ...          ...   ...    ...   ...     ...  \n",
       "68200  0.333333  0.333333          0.0   0.0    1.0   0.0     1.0  \n",
       "68201  0.555556  0.500000          0.5   0.5    0.0   0.0     1.0  \n",
       "68202  1.000000  0.500000          1.0   0.0    0.0   1.0     0.0  \n",
       "68203  0.500000  0.333333          0.0   0.5    0.0   0.0     0.0  \n",
       "68204  0.333333  0.333333          0.5   0.0    0.0   0.0     1.0  \n",
       "\n",
       "[68205 rows x 13 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.drop(columns='age_years', inplace=True)\n",
    "df_combined.drop(columns='bmi', inplace=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 54564\n",
      "Validation set size: 6820\n",
      "Test set size: 6821\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming your combined dataframe is called df_combined\n",
    "# Let's define the features and the target\n",
    "X = df_combined.drop(columns=['bp_category_encoded'])  # Drop the target columns\n",
    "y = df_combined['bp_category_encoded']  # Assuming this is your target column\n",
    "\n",
    "# First split: 80% training, 20% temporary (for validation and testing)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Second split: 50% validation, 50% testing (from the temporary set)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Now you have:\n",
    "# - X_train, y_train: Training set (80% of the original data)\n",
    "# - X_val, y_val: Validation set (10% of the original data)\n",
    "# - X_test, y_test: Test set (10% of the original data)\n",
    "\n",
    "# You can verify the sizes of each split:\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "\n",
    "X_val.to_csv('X_val.csv', index=False)\n",
    "y_val.to_csv('y_val.csv', index=False)\n",
    "\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert DataFrames to NumPy arrays and then to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # For classification, target must be long (int)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset which pairs the inputs and outputs (features and labels)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create DataLoader for batching and shuffling the training set\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create DataLoader for the validation set (no need for shuffling during evaluation)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]  # Number of features\n",
    "output_size = len(y.unique())  # Number of unique classes\n",
    "print(output_size)\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.4344\n",
      "Validation Loss: 0.2712, Validation Accuracy: 91.73%\n",
      "Epoch [2/200], Loss: 0.2493\n",
      "Validation Loss: 0.1490, Validation Accuracy: 95.50%\n",
      "Epoch [3/200], Loss: 0.1182\n",
      "Validation Loss: 0.0551, Validation Accuracy: 98.40%\n",
      "Epoch [4/200], Loss: 0.0726\n",
      "Validation Loss: 0.0469, Validation Accuracy: 99.03%\n",
      "Epoch [5/200], Loss: 0.0623\n",
      "Validation Loss: 0.0404, Validation Accuracy: 98.93%\n",
      "Epoch [6/200], Loss: 0.0585\n",
      "Validation Loss: 0.0431, Validation Accuracy: 98.90%\n",
      "Epoch [7/200], Loss: 0.0563\n",
      "Validation Loss: 0.0336, Validation Accuracy: 99.31%\n",
      "Epoch [8/200], Loss: 0.0492\n",
      "Validation Loss: 0.0338, Validation Accuracy: 99.00%\n",
      "Epoch [9/200], Loss: 0.0485\n",
      "Validation Loss: 0.0281, Validation Accuracy: 99.43%\n",
      "Epoch [10/200], Loss: 0.0461\n",
      "Validation Loss: 0.0285, Validation Accuracy: 99.37%\n",
      "Epoch [11/200], Loss: 0.0443\n",
      "Validation Loss: 0.0261, Validation Accuracy: 99.38%\n",
      "Epoch [12/200], Loss: 0.0412\n",
      "Validation Loss: 0.0251, Validation Accuracy: 99.24%\n",
      "Epoch [13/200], Loss: 0.0393\n",
      "Validation Loss: 0.0194, Validation Accuracy: 99.47%\n",
      "Epoch [14/200], Loss: 0.0365\n",
      "Validation Loss: 0.0209, Validation Accuracy: 99.50%\n",
      "Epoch [15/200], Loss: 0.0376\n",
      "Validation Loss: 0.0219, Validation Accuracy: 99.43%\n",
      "Epoch [16/200], Loss: 0.0324\n",
      "Validation Loss: 0.0201, Validation Accuracy: 99.41%\n",
      "Epoch [17/200], Loss: 0.0320\n",
      "Validation Loss: 0.0212, Validation Accuracy: 99.43%\n",
      "Epoch [18/200], Loss: 0.0302\n",
      "Validation Loss: 0.0404, Validation Accuracy: 99.40%\n",
      "Epoch [19/200], Loss: 0.0290\n",
      "Validation Loss: 0.0199, Validation Accuracy: 99.43%\n",
      "Epoch [20/200], Loss: 0.0216\n",
      "Validation Loss: 0.0122, Validation Accuracy: 99.68%\n",
      "Epoch [21/200], Loss: 0.0197\n",
      "Validation Loss: 0.0115, Validation Accuracy: 99.74%\n",
      "Epoch [22/200], Loss: 0.0196\n",
      "Validation Loss: 0.0131, Validation Accuracy: 99.68%\n",
      "Epoch [23/200], Loss: 0.0203\n",
      "Validation Loss: 0.0136, Validation Accuracy: 99.57%\n",
      "Epoch [24/200], Loss: 0.0197\n",
      "Validation Loss: 0.0290, Validation Accuracy: 99.43%\n",
      "Epoch [25/200], Loss: 0.0199\n",
      "Validation Loss: 0.0123, Validation Accuracy: 99.69%\n",
      "Epoch [26/200], Loss: 0.0174\n",
      "Validation Loss: 0.0085, Validation Accuracy: 99.77%\n",
      "Epoch [27/200], Loss: 0.0177\n",
      "Validation Loss: 0.0169, Validation Accuracy: 99.57%\n",
      "Epoch [28/200], Loss: 0.0169\n",
      "Validation Loss: 0.0168, Validation Accuracy: 99.59%\n",
      "Epoch [29/200], Loss: 0.0155\n",
      "Validation Loss: 0.0123, Validation Accuracy: 99.66%\n",
      "Epoch [30/200], Loss: 0.0164\n",
      "Validation Loss: 0.0079, Validation Accuracy: 99.79%\n",
      "Epoch [31/200], Loss: 0.0172\n",
      "Validation Loss: 0.0107, Validation Accuracy: 99.71%\n",
      "Epoch [32/200], Loss: 0.0151\n",
      "Validation Loss: 0.0085, Validation Accuracy: 99.79%\n",
      "Epoch [33/200], Loss: 0.0157\n",
      "Validation Loss: 0.0107, Validation Accuracy: 99.79%\n",
      "Epoch [34/200], Loss: 0.0161\n",
      "Validation Loss: 0.0097, Validation Accuracy: 99.74%\n",
      "Epoch [35/200], Loss: 0.0147\n",
      "Validation Loss: 0.0070, Validation Accuracy: 99.84%\n",
      "Epoch [36/200], Loss: 0.0140\n",
      "Validation Loss: 0.0081, Validation Accuracy: 99.79%\n",
      "Epoch [37/200], Loss: 0.0150\n",
      "Validation Loss: 0.0103, Validation Accuracy: 99.74%\n",
      "Epoch [38/200], Loss: 0.0147\n",
      "Validation Loss: 0.0062, Validation Accuracy: 99.79%\n",
      "Epoch [39/200], Loss: 0.0138\n",
      "Validation Loss: 0.0096, Validation Accuracy: 99.74%\n",
      "Epoch [40/200], Loss: 0.0146\n",
      "Validation Loss: 0.0107, Validation Accuracy: 99.74%\n",
      "Epoch [41/200], Loss: 0.0133\n",
      "Validation Loss: 0.0106, Validation Accuracy: 99.69%\n",
      "Epoch [42/200], Loss: 0.0135\n",
      "Validation Loss: 0.0097, Validation Accuracy: 99.78%\n",
      "Epoch [43/200], Loss: 0.0133\n",
      "Validation Loss: 0.0084, Validation Accuracy: 99.77%\n",
      "Epoch [44/200], Loss: 0.0122\n",
      "Validation Loss: 0.0094, Validation Accuracy: 99.75%\n",
      "Epoch [45/200], Loss: 0.0101\n",
      "Validation Loss: 0.0055, Validation Accuracy: 99.81%\n",
      "Epoch [46/200], Loss: 0.0100\n",
      "Validation Loss: 0.0059, Validation Accuracy: 99.84%\n",
      "Epoch [47/200], Loss: 0.0096\n",
      "Validation Loss: 0.0081, Validation Accuracy: 99.77%\n",
      "Epoch [48/200], Loss: 0.0100\n",
      "Validation Loss: 0.0045, Validation Accuracy: 99.88%\n",
      "Epoch [49/200], Loss: 0.0091\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.88%\n",
      "Epoch [50/200], Loss: 0.0092\n",
      "Validation Loss: 0.0056, Validation Accuracy: 99.84%\n",
      "Epoch [51/200], Loss: 0.0099\n",
      "Validation Loss: 0.0062, Validation Accuracy: 99.84%\n",
      "Epoch [52/200], Loss: 0.0094\n",
      "Validation Loss: 0.0047, Validation Accuracy: 99.85%\n",
      "Epoch [53/200], Loss: 0.0091\n",
      "Validation Loss: 0.0055, Validation Accuracy: 99.81%\n",
      "Epoch [54/200], Loss: 0.0093\n",
      "Validation Loss: 0.0042, Validation Accuracy: 99.87%\n",
      "Epoch [55/200], Loss: 0.0093\n",
      "Validation Loss: 0.0072, Validation Accuracy: 99.78%\n",
      "Epoch [56/200], Loss: 0.0085\n",
      "Validation Loss: 0.0058, Validation Accuracy: 99.84%\n",
      "Epoch [57/200], Loss: 0.0076\n",
      "Validation Loss: 0.0040, Validation Accuracy: 99.88%\n",
      "Epoch [58/200], Loss: 0.0080\n",
      "Validation Loss: 0.0042, Validation Accuracy: 99.87%\n",
      "Epoch [59/200], Loss: 0.0079\n",
      "Validation Loss: 0.0060, Validation Accuracy: 99.81%\n",
      "Epoch [60/200], Loss: 0.0071\n",
      "Validation Loss: 0.0046, Validation Accuracy: 99.87%\n",
      "Epoch [61/200], Loss: 0.0072\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [62/200], Loss: 0.0077\n",
      "Validation Loss: 0.0039, Validation Accuracy: 99.88%\n",
      "Epoch [63/200], Loss: 0.0073\n",
      "Validation Loss: 0.0057, Validation Accuracy: 99.82%\n",
      "Epoch [64/200], Loss: 0.0075\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [65/200], Loss: 0.0066\n",
      "Validation Loss: 0.0031, Validation Accuracy: 99.90%\n",
      "Epoch [66/200], Loss: 0.0067\n",
      "Validation Loss: 0.0033, Validation Accuracy: 99.90%\n",
      "Epoch [67/200], Loss: 0.0072\n",
      "Validation Loss: 0.0039, Validation Accuracy: 99.87%\n",
      "Epoch [68/200], Loss: 0.0074\n",
      "Validation Loss: 0.0065, Validation Accuracy: 99.82%\n",
      "Epoch [69/200], Loss: 0.0073\n",
      "Validation Loss: 0.0039, Validation Accuracy: 99.87%\n",
      "Epoch [70/200], Loss: 0.0074\n",
      "Validation Loss: 0.0046, Validation Accuracy: 99.84%\n",
      "Epoch [71/200], Loss: 0.0069\n",
      "Validation Loss: 0.0034, Validation Accuracy: 99.90%\n",
      "Epoch [72/200], Loss: 0.0060\n",
      "Validation Loss: 0.0042, Validation Accuracy: 99.85%\n",
      "Epoch [73/200], Loss: 0.0070\n",
      "Validation Loss: 0.0039, Validation Accuracy: 99.88%\n",
      "Epoch [74/200], Loss: 0.0065\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [75/200], Loss: 0.0065\n",
      "Validation Loss: 0.0039, Validation Accuracy: 99.88%\n",
      "Epoch [76/200], Loss: 0.0065\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [77/200], Loss: 0.0062\n",
      "Validation Loss: 0.0040, Validation Accuracy: 99.85%\n",
      "Epoch [78/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [79/200], Loss: 0.0063\n",
      "Validation Loss: 0.0039, Validation Accuracy: 99.87%\n",
      "Epoch [80/200], Loss: 0.0060\n",
      "Validation Loss: 0.0040, Validation Accuracy: 99.87%\n",
      "Epoch [81/200], Loss: 0.0060\n",
      "Validation Loss: 0.0036, Validation Accuracy: 99.88%\n",
      "Epoch [82/200], Loss: 0.0060\n",
      "Validation Loss: 0.0036, Validation Accuracy: 99.88%\n",
      "Epoch [83/200], Loss: 0.0048\n",
      "Validation Loss: 0.0043, Validation Accuracy: 99.87%\n",
      "Epoch [84/200], Loss: 0.0056\n",
      "Validation Loss: 0.0042, Validation Accuracy: 99.88%\n",
      "Epoch [85/200], Loss: 0.0057\n",
      "Validation Loss: 0.0035, Validation Accuracy: 99.88%\n",
      "Epoch [86/200], Loss: 0.0054\n",
      "Validation Loss: 0.0036, Validation Accuracy: 99.88%\n",
      "Epoch [87/200], Loss: 0.0053\n",
      "Validation Loss: 0.0044, Validation Accuracy: 99.87%\n",
      "Epoch [88/200], Loss: 0.0061\n",
      "Validation Loss: 0.0035, Validation Accuracy: 99.88%\n",
      "Epoch [89/200], Loss: 0.0058\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [90/200], Loss: 0.0059\n",
      "Validation Loss: 0.0044, Validation Accuracy: 99.87%\n",
      "Epoch [91/200], Loss: 0.0055\n",
      "Validation Loss: 0.0043, Validation Accuracy: 99.87%\n",
      "Epoch [92/200], Loss: 0.0054\n",
      "Validation Loss: 0.0036, Validation Accuracy: 99.88%\n",
      "Epoch [93/200], Loss: 0.0048\n",
      "Validation Loss: 0.0040, Validation Accuracy: 99.87%\n",
      "Epoch [94/200], Loss: 0.0053\n",
      "Validation Loss: 0.0039, Validation Accuracy: 99.87%\n",
      "Epoch [95/200], Loss: 0.0058\n",
      "Validation Loss: 0.0036, Validation Accuracy: 99.87%\n",
      "Epoch [96/200], Loss: 0.0057\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [97/200], Loss: 0.0054\n",
      "Validation Loss: 0.0040, Validation Accuracy: 99.87%\n",
      "Epoch [98/200], Loss: 0.0058\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [99/200], Loss: 0.0053\n",
      "Validation Loss: 0.0036, Validation Accuracy: 99.88%\n",
      "Epoch [100/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [101/200], Loss: 0.0057\n",
      "Validation Loss: 0.0036, Validation Accuracy: 99.88%\n",
      "Epoch [102/200], Loss: 0.0051\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [103/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [104/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [105/200], Loss: 0.0056\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [106/200], Loss: 0.0050\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [107/200], Loss: 0.0051\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [108/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [109/200], Loss: 0.0047\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [110/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [111/200], Loss: 0.0058\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [112/200], Loss: 0.0049\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [113/200], Loss: 0.0057\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [114/200], Loss: 0.0052\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [115/200], Loss: 0.0055\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [116/200], Loss: 0.0055\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [117/200], Loss: 0.0052\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [118/200], Loss: 0.0057\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [119/200], Loss: 0.0050\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [120/200], Loss: 0.0053\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [121/200], Loss: 0.0054\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [122/200], Loss: 0.0052\n",
      "Validation Loss: 0.0038, Validation Accuracy: 99.87%\n",
      "Epoch [123/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [124/200], Loss: 0.0052\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [125/200], Loss: 0.0050\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [126/200], Loss: 0.0050\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [127/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [128/200], Loss: 0.0057\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [129/200], Loss: 0.0052\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [130/200], Loss: 0.0060\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.87%\n",
      "Epoch [131/200], Loss: 0.0057\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [132/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [133/200], Loss: 0.0050\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [134/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [135/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [136/200], Loss: 0.0058\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [137/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [138/200], Loss: 0.0056\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [139/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [140/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [141/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [142/200], Loss: 0.0052\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [143/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [144/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [145/200], Loss: 0.0057\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [146/200], Loss: 0.0050\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [147/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [148/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [149/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [150/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [151/200], Loss: 0.0059\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [152/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [153/200], Loss: 0.0049\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [154/200], Loss: 0.0056\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [155/200], Loss: 0.0056\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [156/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [157/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [158/200], Loss: 0.0052\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [159/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [160/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [161/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [162/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [163/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [164/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [165/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [166/200], Loss: 0.0048\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [167/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [168/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [169/200], Loss: 0.0056\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [170/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [171/200], Loss: 0.0050\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [172/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [173/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [174/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [175/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [176/200], Loss: 0.0057\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [177/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [178/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [179/200], Loss: 0.0052\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [180/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [181/200], Loss: 0.0056\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [182/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [183/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [184/200], Loss: 0.0052\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [185/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [186/200], Loss: 0.0054\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [187/200], Loss: 0.0052\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [188/200], Loss: 0.0056\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [189/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [190/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [191/200], Loss: 0.0058\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [192/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [193/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [194/200], Loss: 0.0055\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [195/200], Loss: 0.0057\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [196/200], Loss: 0.0053\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [197/200], Loss: 0.0049\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [198/200], Loss: 0.0050\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [199/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n",
      "Epoch [200/200], Loss: 0.0051\n",
      "Validation Loss: 0.0037, Validation Accuracy: 99.88%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),  # Lower dropout\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),         # Consistent activation\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),\n",
    "    nn.Linear(16, output_size)  # No Softmax here\n",
    ")\n",
    "\n",
    "import os\n",
    "model_save_dir = 'model'\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Higher learning rate\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training step\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    correct, total, val_loss = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save the model checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'{model_save_dir}/model_{epoch+1}.pt')\n",
    "\n",
    "# Save the final trained model\n",
    "torch.save(model.state_dict(), f'{model_save_dir}/final_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11028"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rifat\\AppData\\Local\\Temp\\ipykernel_26240\\3574674988.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'{model_save_dir}/final_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.84%\n",
      "\n",
      "Per-Class Metrics:\n",
      "Class 0 - Precision: 0.9992, Recall: 0.9985, F1-Score: 0.9989\n",
      "Class 1 - Precision: 0.9962, Recall: 0.9987, F1-Score: 0.9975\n",
      "Class 2 - Precision: 1.0000, Recall: 0.9979, F1-Score: 0.9990\n",
      "Class 3 - Precision: 0.9935, Recall: 0.9968, F1-Score: 0.9952\n",
      "Multi-Class ROC-AUC (OVR): 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model (same architecture as trained)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),  # Lower dropout\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),         # Consistent activation\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),\n",
    "    nn.Linear(16, output_size)  # No Softmax here\n",
    ")\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load(f'{model_save_dir}/final_model.pt'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "model.to(device)\n",
    "\n",
    "# Test data preparation\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize variables to store results\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_probs = []  # To store probabilities for metrics like ROC-AUC\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Predicted labels\n",
    "        probabilities = torch.softmax(outputs, dim=1)  # Probabilities for each class\n",
    "\n",
    "        # Append results to lists\n",
    "        y_true.extend(labels.cpu().numpy())  # Ground truth\n",
    "        y_pred.extend(predicted.cpu().numpy())  # Predicted labels\n",
    "        y_probs.extend(probabilities.cpu().numpy())  # Class probabilities\n",
    "\n",
    "# Convert results to NumPy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_probs = np.array(y_probs)  # Probabilities for each class\n",
    "\n",
    "# **1. Classification Report**\n",
    "print(\"\\nClassification Report:\")\n",
    "\n",
    "# **2. Confusion Matrix**\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\", ..., f\"Class {output_size-1}\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# **3. Accuracy**\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# **4. Precision, Recall, and F1-Score**\n",
    "precision = cm.diagonal() / cm.sum(axis=0)\n",
    "recall = cm.diagonal() / cm.sum(axis=1)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for i, (p, r, f1) in enumerate(zip(precision, recall, f1_scores)):\n",
    "    print(f\"Class {i} - Precision: {p:.4f}, Recall: {r:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# **5. ROC-AUC (for multi-class)**\n",
    "if output_size == 2:\n",
    "    # For binary classification\n",
    "    roc_auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs[:, 1])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    # For multi-class ROC-AUC (one-vs-rest approach)\n",
    "    roc_auc_ovr = roc_auc_score(y_true, y_probs, multi_class='ovr')\n",
    "    print(f\"Multi-Class ROC-AUC (OVR): {roc_auc_ovr:.4f}\")\n",
    "\n",
    "# **6. Log Predictions (optional for debugging)**\n",
    "# Save the predictions for analysis\n",
    "np.savetxt(\"predictions.csv\", np.column_stack((y_true, y_pred)), fmt='%d', delimiter=',', header='True,Predicted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.20.1-cp311-cp311-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\rifat\\miniconda3\\envs\\mlweb\\lib\\site-packages (from onnxruntime) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\rifat\\miniconda3\\envs\\mlweb\\lib\\site-packages (from onnxruntime) (24.2)\n",
      "Collecting protobuf (from onnxruntime)\n",
      "  Downloading protobuf-5.28.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: sympy in c:\\users\\rifat\\miniconda3\\envs\\mlweb\\lib\\site-packages (from onnxruntime) (1.13.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rifat\\miniconda3\\envs\\mlweb\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading onnxruntime-1.20.1-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 16.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.3 MB 17.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.7/11.3 MB 16.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading protobuf-5.28.3-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: flatbuffers, pyreadline3, protobuf, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-24.3.25 humanfriendly-10.0 onnxruntime-1.20.1 protobuf-5.28.3 pyreadline3-3.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from model/final_model.onnx failed:Load model model/final_model.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the ONNX model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_model.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Get the input and output names from the ONNX model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m input_name \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\mlweb\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:465\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[1;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\miniconda3\\envs\\mlweb\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:526\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[1;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[1;32m--> 526\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    528\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[1;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from model/final_model.onnx failed:Load model model/final_model.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = 'model/final_model.onnx'\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Get the input and output names from the ONNX model\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "print(f\"Input name: {input_name}, Output name: {output_name}\")\n",
    "\n",
    "# Prepare the test data (use the same test data you used for PyTorch)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).cpu().numpy()\n",
    "y_test_tensor = y_test.values\n",
    "\n",
    "# Initialize variables to store results\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_probs = []\n",
    "\n",
    "# Evaluate the model on the test set using ONNX Runtime\n",
    "for i in range(0, len(X_test_tensor), 64):  # Assuming batch size of 64\n",
    "    batch_input = X_test_tensor[i:i+64]\n",
    "    batch_labels = y_test_tensor[i:i+64]\n",
    "    \n",
    "    # Run inference on the batch\n",
    "    outputs = session.run([output_name], {input_name: batch_input.astype(np.float32)})\n",
    "    \n",
    "    # Get the predictions (class with max probability)\n",
    "    predicted = np.argmax(outputs[0], axis=1)\n",
    "    probabilities = outputs[0]  # Raw probabilities for each class\n",
    "    \n",
    "    # Append results to lists\n",
    "    y_true.extend(batch_labels)\n",
    "    y_pred.extend(predicted)\n",
    "    y_probs.extend(probabilities)\n",
    "\n",
    "# Convert results to NumPy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_probs = np.array(y_probs)\n",
    "\n",
    "# **1. Classification Report**\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Class 0\", \"Class 1\", ..., f\"Class {output_size-1}\"]))\n",
    "\n",
    "# **2. Confusion Matrix**\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\", ..., f\"Class {output_size-1}\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# **3. Accuracy**\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# **4. Precision, Recall, and F1-Score**\n",
    "precision = cm.diagonal() / cm.sum(axis=0)\n",
    "recall = cm.diagonal() / cm.sum(axis=1)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for i, (p, r, f1) in enumerate(zip(precision, recall, f1_scores)):\n",
    "    print(f\"Class {i} - Precision: {p:.4f}, Recall: {r:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# **5. ROC-AUC (for multi-class)**\n",
    "if output_size == 2:\n",
    "    # For binary classification\n",
    "    roc_auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs[:, 1])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    # For multi-class ROC-AUC (one-vs-rest approach)\n",
    "    roc_auc_ovr = roc_auc_score(y_true, y_probs, multi_class='ovr')\n",
    "    print(f\"Multi-Class ROC-AUC (OVR): {roc_auc_ovr:.4f}\")\n",
    "\n",
    "# **6. Log Predictions (optional for debugging)**\n",
    "np.savetxt(\"predictions.csv\", np.column_stack((y_true, y_pred)), fmt='%d', delimiter=',', header='True,Predicted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = 'final_model.onnx'\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Get the input and output names from the ONNX model\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "print(f\"Input name: {input_name}, Output name: {output_name}\")\n",
    "\n",
    "# Prepare the test data (use the same test data you used for PyTorch)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).cpu().numpy()\n",
    "y_test_tensor = y_test.values\n",
    "\n",
    "# Initialize variables to store results\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_probs = []\n",
    "\n",
    "# Evaluate the model on the test set using ONNX Runtime\n",
    "for i in range(0, len(X_test_tensor), 64):  # Assuming batch size of 64\n",
    "    batch_input = X_test_tensor[i:i+64]\n",
    "    batch_labels = y_test_tensor[i:i+64]\n",
    "    \n",
    "    # Run inference on the batch\n",
    "    outputs = session.run([output_name], {input_name: batch_input.astype(np.float32)})\n",
    "    \n",
    "    # Get the predictions (class with max probability)\n",
    "    predicted = np.argmax(outputs[0], axis=1)\n",
    "    probabilities = outputs[0]  # Raw probabilities for each class\n",
    "    \n",
    "    # Append results to lists\n",
    "    y_true.extend(batch_labels)\n",
    "    y_pred.extend(predicted)\n",
    "    y_probs.extend(probabilities)\n",
    "\n",
    "# Convert results to NumPy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_probs = np.array(y_probs)\n",
    "\n",
    "# **1. Classification Report**\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Class 0\", \"Class 1\", ..., f\"Class {output_size-1}\"]))\n",
    "\n",
    "# **2. Confusion Matrix**\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Class 0\", \"Class 1\", ..., f\"Class {output_size-1}\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# **3. Accuracy**\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# **4. Precision, Recall, and F1-Score**\n",
    "precision = cm.diagonal() / cm.sum(axis=0)\n",
    "recall = cm.diagonal() / cm.sum(axis=1)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for i, (p, r, f1) in enumerate(zip(precision, recall, f1_scores)):\n",
    "    print(f\"Class {i} - Precision: {p:.4f}, Recall: {r:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# **5. ROC-AUC (for multi-class)**\n",
    "if output_size == 2:\n",
    "    # For binary classification\n",
    "    roc_auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs[:, 1])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    # For multi-class ROC-AUC (one-vs-rest approach)\n",
    "    roc_auc_ovr = roc_auc_score(y_true, y_probs, multi_class='ovr')\n",
    "    print(f\"Multi-Class ROC-AUC (OVR): {roc_auc_ovr:.4f}\")\n",
    "\n",
    "# **6. Log Predictions (optional for debugging)**\n",
    "np.savetxt(\"predictions.csv\", np.column_stack((y_true, y_pred)), fmt='%d', delimiter=',', header='True,Predicted')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
