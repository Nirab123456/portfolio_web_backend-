{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=11\n",
    "output_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import os\n",
    "from torch import nn\n",
    "\n",
    "# Load the trained model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),  # Lower dropout\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),         # Consistent activation\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),\n",
    "    nn.Linear(16, output_size)  # No Softmax here\n",
    ")\n",
    "\n",
    "# Load weights from the trained model\n",
    "model.load_state_dict(torch.load('final_model.pt'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Specify a dummy input with the correct shape (e.g., input_size features)\n",
    "dummy_input = torch.randn(1, input_size)  # Batch size 1, with input_size features\n",
    "\n",
    "# Path to save the ONNX model\n",
    "onnx_model_path = 'final_model.onnx'\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path, export_params=True, opset_version=12,\n",
    "                  do_constant_folding=True, input_names=['input'], output_names=['output'])\n",
    "\n",
    "print(f\"Model has been exported to {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the number of classes (output size)\n",
    "output_size = 4  # Number of output classes (adjust this based on your model's output)\n",
    "\n",
    "# Define the class labels (adjust based on your dataset)\n",
    "class_labels = [f\"Class {i}\" for i in range(output_size)]\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = 'final_model.onnx'\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Get the input and output names from the ONNX model\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "print(f\"Input name: {input_name}, Output name: {output_name}\")\n",
    "\n",
    "# Prepare the test data (assuming `X_test` and `y_test` are pandas DataFrames or numpy arrays)\n",
    "X_test_tensor = X_test.values.astype(np.float32)  # Convert to numpy array if it's a pandas DataFrame\n",
    "y_test_tensor = y_test.values  # Actual labels for the test set\n",
    "\n",
    "# Initialize variables to store results\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_probs = []  # To store probabilities for metrics like ROC-AUC\n",
    "\n",
    "# Evaluate the model on the test set using ONNX Runtime\n",
    "batch_size = 1 # You can adjust the batch size here\n",
    "for i in range(0, len(X_test_tensor), batch_size):  # Iterate through batches\n",
    "    batch_input = X_test_tensor[i:i+batch_size]\n",
    "    batch_labels = y_test_tensor[i:i+batch_size]\n",
    "    \n",
    "    # Run inference on the batch\n",
    "    outputs = session.run([output_name], {input_name: batch_input})\n",
    "    \n",
    "    # Get the predicted labels (class with max probability)\n",
    "    predicted = np.argmax(outputs[0], axis=1)  # Get the class with the max probability\n",
    "    probabilities = outputs[0]  # Raw probabilities for each class\n",
    "    \n",
    "    # Append results to lists\n",
    "    y_true.extend(batch_labels)\n",
    "    y_pred.extend(predicted)\n",
    "    y_probs.extend(probabilities)\n",
    "\n",
    "# Convert results to NumPy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_probs = np.array(y_probs)  # Probabilities for each class\n",
    "\n",
    "# **1. Classification Report**\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_labels))\n",
    "\n",
    "# **2. Confusion Matrix**\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# **3. Accuracy**\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# **4. Precision, Recall, and F1-Score**\n",
    "precision = cm.diagonal() / cm.sum(axis=0)\n",
    "recall = cm.diagonal() / cm.sum(axis=1)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for i, (p, r, f1) in enumerate(zip(precision, recall, f1_scores)):\n",
    "    print(f\"Class {i} - Precision: {p:.4f}, Recall: {r:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# **5. ROC-AUC (for multi-class)**\n",
    "if output_size == 2:\n",
    "    # For binary classification\n",
    "    roc_auc = roc_auc_score(y_true, y_probs[:, 1])\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs[:, 1])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    # For multi-class ROC-AUC (one-vs-rest approach)\n",
    "    roc_auc_ovr = roc_auc_score(y_true, y_probs, multi_class='ovr')\n",
    "    print(f\"Multi-Class ROC-AUC (OVR): {roc_auc_ovr:.4f}\")\n",
    "\n",
    "# **6. Log Predictions (optional for debugging)**\n",
    "np.savetxt(\"predictions.csv\", np.column_stack((y_true, y_pred)), fmt='%d', delimiter=',', header='True,Predicted')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
